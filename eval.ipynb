{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1:\n",
      "[Score(precision=0.8571428571428571, recall=1.0, fmeasure=0.923076923076923), Score(precision=0.7142857142857143, recall=0.8333333333333334, fmeasure=0.7692307692307692)]\n",
      "rouge2:\n",
      "[Score(precision=0.6666666666666666, recall=0.8, fmeasure=0.7272727272727272), Score(precision=0.3333333333333333, recall=0.4, fmeasure=0.3636363636363636)]\n",
      "rougeL:\n",
      "[Score(precision=0.8571428571428571, recall=1.0, fmeasure=0.923076923076923), Score(precision=0.5714285714285714, recall=0.6666666666666666, fmeasure=0.6153846153846153)]\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "candidate_summary = \"the cat was found under the bed\"\n",
    "reference_summaries = [\"the cat was under the bed\", \"found a cat under the bed\"]\n",
    "scores = {key: [] for key in ['rouge1', 'rouge2', 'rougeL']}\n",
    "for ref in reference_summaries:\n",
    "    temp_scores = scorer.score(ref, candidate_summary)\n",
    "    for key in temp_scores:\n",
    "        scores[key].append(temp_scores[key])\n",
    "\n",
    "for key in scores:\n",
    "    print(f'{key}:\\n{scores[key]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m reference_sentences \u001b[38;5;241m=\u001b[39m array_train_dataset\n\u001b[0;32m     12\u001b[0m gpt2_tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(gpt2_output)\n\u001b[1;32m---> 13\u001b[0m reference_tokens \u001b[38;5;241m=\u001b[39m [nltk\u001b[38;5;241m.\u001b[39mword_tokenize(sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m reference_sentences]\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Calculate BLEU score with detailed n-gram precision\u001b[39;00m\n\u001b[0;32m     16\u001b[0m bleu_score, individual_ngram_precisions \u001b[38;5;241m=\u001b[39m sentence_bleu(reference_tokens, gpt2_tokens, smoothing_function\u001b[38;5;241m=\u001b[39mSmoothingFunction()\u001b[38;5;241m.\u001b[39mmethod1, weights\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.25\u001b[39m, \u001b[38;5;241m0.25\u001b[39m, \u001b[38;5;241m0.25\u001b[39m, \u001b[38;5;241m0.25\u001b[39m))\n",
      "Cell \u001b[1;32mIn[7], line 13\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     10\u001b[0m reference_sentences \u001b[38;5;241m=\u001b[39m array_train_dataset\n\u001b[0;32m     12\u001b[0m gpt2_tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(gpt2_output)\n\u001b[1;32m---> 13\u001b[0m reference_tokens \u001b[38;5;241m=\u001b[39m [\u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m reference_sentences]\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Calculate BLEU score with detailed n-gram precision\u001b[39;00m\n\u001b[0;32m     16\u001b[0m bleu_score, individual_ngram_precisions \u001b[38;5;241m=\u001b[39m sentence_bleu(reference_tokens, gpt2_tokens, smoothing_function\u001b[38;5;241m=\u001b[39mSmoothingFunction()\u001b[38;5;241m.\u001b[39mmethod1, weights\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.25\u001b[39m, \u001b[38;5;241m0.25\u001b[39m, \u001b[38;5;241m0.25\u001b[39m, \u001b[38;5;241m0.25\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\asus\\anaconda3\\envs\\Web-Transformers-Nvidia\\lib\\site-packages\\nltk\\tokenize\\__init__.py:130\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\asus\\anaconda3\\envs\\Web-Transformers-Nvidia\\lib\\site-packages\\nltk\\tokenize\\__init__.py:131\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 131\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_treebank_word_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\asus\\anaconda3\\envs\\Web-Transformers-Nvidia\\lib\\site-packages\\nltk\\tokenize\\destructive.py:164\u001b[0m, in \u001b[0;36mNLTKWordTokenizer.tokenize\u001b[1;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;66;03m# Handles parentheses.\u001b[39;00m\n\u001b[0;32m    163\u001b[0m regexp, substitution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPARENS_BRACKETS\n\u001b[1;32m--> 164\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mregexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubstitution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;66;03m# Optionally convert parentheses\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_parentheses:\n",
      "File \u001b[1;32mc:\\Users\\asus\\anaconda3\\envs\\Web-Transformers-Nvidia\\lib\\re.py:331\u001b[0m, in \u001b[0;36m_subx.<locals>.filter\u001b[1;34m(match, template)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfilter\u001b[39m(match, template\u001b[38;5;241m=\u001b[39mtemplate):\n\u001b[1;32m--> 331\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msre_parse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\asus\\anaconda3\\envs\\Web-Transformers-Nvidia\\lib\\sre_parse.py:1073\u001b[0m, in \u001b[0;36mexpand_template\u001b[1;34m(template, match)\u001b[0m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index, group \u001b[38;5;129;01min\u001b[39;00m groups:\n\u001b[1;32m-> 1073\u001b[0m         literals[index] \u001b[38;5;241m=\u001b[39m g(group) \u001b[38;5;129;01mor\u001b[39;00m empty\n\u001b[0;32m   1074\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid group reference \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m index)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "file_path = \"train_translated_output.txt\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    array_train_dataset = f.read().splitlines()\n",
    "\n",
    "gpt2_output = \"Saya ingin membeli mobil baru. Orang tua saya mengatakan kepada saya bahwa mereka tidak bisa membelinya. Saya memutuskan untuk pergi ke dealer di kota lain. Di sana, orang-orang sangat ramah dan murah hati. Mobil itu adalah hadiah ulang tahun yang sempurna!\"\n",
    "\n",
    "reference_sentences = array_train_dataset\n",
    "\n",
    "gpt2_tokens = nltk.word_tokenize(gpt2_output)\n",
    "reference_tokens = [nltk.word_tokenize(sentence) for sentence in reference_sentences]\n",
    "\n",
    "# Calculate BLEU score with detailed n-gram precision\n",
    "bleu_score, individual_ngram_precisions = sentence_bleu(reference_tokens, gpt2_tokens, smoothing_function=SmoothingFunction().method1, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "\n",
    "print(\"BLEU score:\", bleu_score)\n",
    "\n",
    "# Print detailed n-gram precision\n",
    "for i, precision in enumerate(individual_ngram_precisions, start=1):\n",
    "    print(f\"N-{i} gram precision:\", precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_24100\\4144741119.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"bleu\")\n",
      "Downloading builder script: 6.06kB [00:00, 3.06MB/s]                   \n",
      "Downloading extra modules: 4.07kB [00:00, 4.07MB/s]                   \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Got a string but expected a list instead: 'hello world'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Text generation using GPT-2 (new dataset)\\eval.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Text%20generation%20using%20GPT-2%20%28new%20dataset%29/eval.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_metric\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Text%20generation%20using%20GPT-2%20%28new%20dataset%29/eval.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m metric \u001b[39m=\u001b[39m load_metric(\u001b[39m\"\u001b[39m\u001b[39mbleu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Text%20generation%20using%20GPT-2%20%28new%20dataset%29/eval.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m metric\u001b[39m.\u001b[39;49mcompute(predictions\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mhello world\u001b[39;49m\u001b[39m\"\u001b[39;49m], references\u001b[39m=\u001b[39;49m[[\u001b[39m\"\u001b[39;49m\u001b[39mhello world\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mhello\u001b[39;49m\u001b[39m\"\u001b[39;49m]])\n",
      "File \u001b[1;32mc:\\Users\\asus\\anaconda3\\envs\\web-Pipeline\\lib\\site-packages\\datasets\\metric.py:442\u001b[0m, in \u001b[0;36mMetric.compute\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    439\u001b[0m compute_kwargs \u001b[39m=\u001b[39m {k: kwargs[k] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m kwargs \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures}\n\u001b[0;32m    441\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(v \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m--> 442\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_batch(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[0;32m    443\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_finalize()\n\u001b[0;32m    445\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache_file_name \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\asus\\anaconda3\\envs\\web-Pipeline\\lib\\site-packages\\datasets\\metric.py:494\u001b[0m, in \u001b[0;36mMetric.add_batch\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    492\u001b[0m batch \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mpredictions\u001b[39m\u001b[39m\"\u001b[39m: predictions, \u001b[39m\"\u001b[39m\u001b[39mreferences\u001b[39m\u001b[39m\"\u001b[39m: references, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs}\n\u001b[0;32m    493\u001b[0m batch \u001b[39m=\u001b[39m {intput_name: batch[intput_name] \u001b[39mfor\u001b[39;00m intput_name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures}\n\u001b[1;32m--> 494\u001b[0m batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfo\u001b[39m.\u001b[39;49mfeatures\u001b[39m.\u001b[39;49mencode_batch(batch)\n\u001b[0;32m    495\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_writer()\n",
      "File \u001b[1;32mc:\\Users\\asus\\anaconda3\\envs\\web-Pipeline\\lib\\site-packages\\datasets\\features\\features.py:1858\u001b[0m, in \u001b[0;36mFeatures.encode_batch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m   1856\u001b[0m \u001b[39mfor\u001b[39;00m key, column \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems():\n\u001b[0;32m   1857\u001b[0m     column \u001b[39m=\u001b[39m cast_to_python_objects(column)\n\u001b[1;32m-> 1858\u001b[0m     encoded_batch[key] \u001b[39m=\u001b[39m [encode_nested_example(\u001b[39mself\u001b[39m[key], obj) \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m column]\n\u001b[0;32m   1859\u001b[0m \u001b[39mreturn\u001b[39;00m encoded_batch\n",
      "File \u001b[1;32mc:\\Users\\asus\\anaconda3\\envs\\web-Pipeline\\lib\\site-packages\\datasets\\features\\features.py:1858\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1856\u001b[0m \u001b[39mfor\u001b[39;00m key, column \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems():\n\u001b[0;32m   1857\u001b[0m     column \u001b[39m=\u001b[39m cast_to_python_objects(column)\n\u001b[1;32m-> 1858\u001b[0m     encoded_batch[key] \u001b[39m=\u001b[39m [encode_nested_example(\u001b[39mself\u001b[39;49m[key], obj) \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m column]\n\u001b[0;32m   1859\u001b[0m \u001b[39mreturn\u001b[39;00m encoded_batch\n",
      "File \u001b[1;32mc:\\Users\\asus\\anaconda3\\envs\\web-Pipeline\\lib\\site-packages\\datasets\\features\\features.py:1262\u001b[0m, in \u001b[0;36mencode_nested_example\u001b[1;34m(schema, obj, level)\u001b[0m\n\u001b[0;32m   1260\u001b[0m \u001b[39m# schema.feature is not a dict\u001b[39;00m\n\u001b[0;32m   1261\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, \u001b[39mstr\u001b[39m):  \u001b[39m# don't interpret a string as a list\u001b[39;00m\n\u001b[1;32m-> 1262\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot a string but expected a list instead: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mobj\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1263\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1264\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(obj) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: Got a string but expected a list instead: 'hello world'"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric(\"bleu\")\n",
    "metric.compute(predictions=[\"hello world\"], references=[[\"hello world\", \"hello\"]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.22111541438690566\n",
      "Individual 1-gram: 0.370409\n",
      "Individual 2-gram: 0.329253\n",
      "Individual 3-gram: 0.185205\n",
      "Individual 4-gram: 0.105831\n",
      "Cumulative 2-gram: 0.349225\n",
      "Cumulative 3-gram: 0.285412\n",
      "Cumulative 4-gram: 0.221115\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from rouge import Rouge\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "file_path = \"train_translated_output.txt\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    array_train_dataset = f.read().splitlines()\n",
    "\n",
    "gpt2_output = \"saya saya saya sekarang sekarang berangkat ke kantor dari rumah\"\n",
    "\n",
    "reference_sentences = [\"saya sekarang berangkat ke kantor dengan mengendarai mobil pada pukul setengah tujuh pagi\"]\n",
    "\n",
    "gpt2_tokens = nltk.word_tokenize(gpt2_output)\n",
    "reference_tokens = [nltk.word_tokenize(sentence) for sentence in reference_sentences]\n",
    "\n",
    "\n",
    "bleu_score = sentence_bleu(reference_tokens, gpt2_tokens)\n",
    "\n",
    "print(\"BLEU score:\", bleu_score)\n",
    "\n",
    "print('Individual 1-gram: %f' % sentence_bleu(reference_tokens, gpt2_tokens, weights=(1, 0, 0, 0)))\n",
    "print('Individual 2-gram: %f' % sentence_bleu(reference_tokens, gpt2_tokens, weights=(0, 1, 0, 0)))\n",
    "print('Individual 3-gram: %f' % sentence_bleu(reference_tokens, gpt2_tokens, weights=(0, 0, 1, 0)))\n",
    "print('Individual 4-gram: %f' % sentence_bleu(reference_tokens, gpt2_tokens, weights=(0, 0, 0, 1)))\n",
    "print('Cumulative 2-gram: %f' % sentence_bleu(reference_tokens, gpt2_tokens, weights=(0.5, 0.5, 0, 0)))\n",
    "print('Cumulative 3-gram: %f' % sentence_bleu(reference_tokens, gpt2_tokens, weights=(0.33, 0.33, 0.33, 0)))\n",
    "print('Cumulative 4-gram: %f' % sentence_bleu(reference_tokens, gpt2_tokens, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 1.0270193092081295e-77\n",
      "Individual 1-gram: 1.000000\n",
      "Individual 2-gram: 1.000000\n",
      "Individual 3-gram: 0.500000\n",
      "Individual 4-gram: 0.000000\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [\n",
    "    'this is a dog'.split(),\n",
    "    'it is dog'.split(),\n",
    "    'dog it is'.split(),\n",
    "    'a dog, it is'.split() \n",
    "]\n",
    "candidate = 'it is a dog'.split()\n",
    "\n",
    "bleu_score = sentence_bleu(reference, candidate)\n",
    "print(\"BLEU score:\", bleu_score)\n",
    "print('Individual 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\n",
    "print('Individual 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 1, 0, 0)))\n",
    "print('Individual 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 1, 0)))\n",
    "print('Individual 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 0, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 4.6144530849934175e-155\n",
      "Individual 1-gram: 0.367879\n",
      "Individual 2-gram: 0.183940\n",
      "Individual 3-gram: 0.000000\n",
      "Individual 4-gram: 0.000000\n",
      "Cumulative 2-gram: 0.260130\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from rouge import Rouge\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "file_path = \"train_translated_output.txt\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    array_train_dataset = f.read().splitlines()\n",
    "\n",
    "gpt2_output = \"the the cat\"\n",
    "\n",
    "reference_sentences = [\"the cat is on the mat\", \"there is a cat on the mat\"]\n",
    "\n",
    "gpt2_tokens = nltk.word_tokenize(gpt2_output)\n",
    "reference_tokens = [nltk.word_tokenize(sentence) for sentence in reference_sentences]\n",
    "\n",
    "\n",
    "bleu_score = sentence_bleu(reference_tokens, gpt2_tokens)\n",
    "\n",
    "print(\"BLEU score:\", bleu_score)\n",
    "\n",
    "print('Individual 1-gram: %f' % sentence_bleu(reference_tokens, gpt2_tokens, weights=(1, 0, 0, 0)))\n",
    "print('Individual 2-gram: %f' % sentence_bleu(reference_tokens, gpt2_tokens, weights=(0, 1, 0, 0)))\n",
    "print('Individual 3-gram: %f' % sentence_bleu(reference_tokens, gpt2_tokens, weights=(0, 0, 1, 0)))\n",
    "print('Individual 4-gram: %f' % sentence_bleu(reference_tokens, gpt2_tokens, weights=(0, 0, 0, 1)))\n",
    "print('Cumulative 2-gram: %f' % sentence_bleu(reference_tokens, gpt2_tokens, weights=(0.5, 0.5, 0, 0)))\n",
    "print('Cumulative 3-gram: %f' % sentence_bleu(reference_tokens, gpt2_tokens, weights=(0.33, 0.33, 0.33, 0)))\n",
    "print('Cumulative 4-gram: %f' % sentence_bleu(reference_tokens, gpt2_tokens, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rouge'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrouge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Rouge\n\u001b[0;32m      4\u001b[0m gpt2_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeorang putri muda yang bernama Luna hidup dalam sebuah kerajaan besar yang dipimpin oleh raja dan ratu.Seorang putri muda yang bernama Luna hidup dalam sebuah kerajaan besar yang dipimpin oleh raja dan ratu. Luna adalah seorang gadis yang sangat menyukai petualangan. Suatu hari, dia mendengar kabar tentang sebuah hutan terlarang yang belum pernah dijelajahi oleh manusia sebelumnya. Dia merasa tertarik dan memutuskan untuk menjelajahi hutan itu sendirian. Namun, ketika dia tiba di sana, sebuah badai menghadang di depan matanya dan memaksanya untuk bertahan hidup dengan cara yang paling aman dan terbaik yang bisa dia lakukan.  Seorang wanita muda bernama Maya baru saja lulus kuliah dan memulai pekerjaan barunya sebagai editor buku di sebuah penerbit terkenal. Maya sedang mencari penerbit yang tepat untuk menerbitkan bukunya, tetapi dia tidak tahu harus mulai dari mana. Hal ini menyebabkan dia merasa kesulitan untuk memulai bisnis barunya dan pada akhirnya membuatnya merasa semakin tertekan dan semakin sulit untuk fokus pada pekerjaannya saat dia terus-menerus merasa gugup dan cemas setiap kali dia melangkah keluar dari gedung penerbitnya.  Maya segera menyadari bahwa ada sesuatu yang salah dengan dirinya dan dia harus berhati-hati dalam mengambil keputusan. Sesuatu yang tidak dia ketahui tentang dunia luar biasanya dan kehidupan orang-orang yang tinggal di dalamnya juga berada di luar jangkauannya. Selain itu, Maya juga mulai merasa takut dan terancam oleh makhluk-makhluk gaib yang berkeliaran di sekitar tempat dia tinggal dan berusaha untuk menguasai mereka. Untuk membantu mengatasi masalah-masalah yang dihadapi Maya, seorang penulisSeorang putri muda yang bernama Luna hidup dalam sebuah kerajaan besar yang dipimpin oleh raja dan ratu.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m reference_sentences \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeorang putri muda yang bernama Luna hidup dalam sebuah kerajaan besar yang dipimpin oleh raja dan ratu.Seorang putri muda yang bernama Luna hidup dalam sebuah kerajaan besar yang dipimpin oleh raja dan ratu. Luna adalah seorang gadis yang sangat menyukai petualangan. Suatu hari, dia mendengar kabar tentang sebuah hutan terlarang yang belum pernah dijelajahi oleh manusia sebelumnya. Dia merasa tertarik dan memutuskan untuk menjelajahi hutan itu sendirian.  Namun, ketika dia tiba di sana, sebuah badai menghadang di depan matanya dan memaksanya untuk bertahan hidup dengan cara yang paling aman dan terbaik yang bisa dia lakukan.  Seorang wanita muda bernama Maya baru saja lulus kuliah dan memulai pekerjaan barunya sebagai editor buku di sebuah penerbit terkenal. Maya sedang mencari penerbit yang tepat untuk menerbitkan bukunya, tetapi dia tidak tahu harus mulai dari mana. Hal ini menyebabkan dia merasa kesulitan untuk memulai bisnis barunya dan pada akhirnya membuatnya merasa semakin tertekan dan semakin sulit untuk fokus pada pekerjaannya saat dia terus-menerus merasa gugup dan cemas setiap kali dia melangkah keluar dari gedung penerbitnya.  Maya segera menyadari bahwa ada sesuatu yang salah dengan dirinya dan dia harus berhati-hati dalam mengambil keputusan. Sesuatu yang tidak dia ketahui tentang dunia luar biasanya dan kehidupan orang-orang yang tinggal di dalamnya juga berada di luar jangkauannya. Selain itu, Maya juga mulai merasa takut dan terancam oleh makhluk-makhluk gaib yang berkeliaran di sekitar tempat dia tinggal dan berusaha untuk menguasai mereka. Untuk membantu mengatasi masalah-masalah yang dihadapi Maya, seorang penulisSeorang putri muda yang bernama Luna hidup dalam sebuah kerajaan besar yang dipimpin oleh raja dan ratu.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'rouge'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from rouge import Rouge\n",
    "\n",
    "gpt2_output = \"Seorang putri muda yang bernama Luna hidup dalam sebuah kerajaan besar yang dipimpin oleh raja dan ratu.Seorang putri muda yang bernama Luna hidup dalam sebuah kerajaan besar yang dipimpin oleh raja dan ratu. Luna adalah seorang gadis yang sangat menyukai petualangan. Suatu hari, dia mendengar kabar tentang sebuah hutan terlarang yang belum pernah dijelajahi oleh manusia sebelumnya. Dia merasa tertarik dan memutuskan untuk menjelajahi hutan itu sendirian. Namun, ketika dia tiba di sana, sebuah badai menghadang di depan matanya dan memaksanya untuk bertahan hidup dengan cara yang paling aman dan terbaik yang bisa dia lakukan.  Seorang wanita muda bernama Maya baru saja lulus kuliah dan memulai pekerjaan barunya sebagai editor buku di sebuah penerbit terkenal. Maya sedang mencari penerbit yang tepat untuk menerbitkan bukunya, tetapi dia tidak tahu harus mulai dari mana. Hal ini menyebabkan dia merasa kesulitan untuk memulai bisnis barunya dan pada akhirnya membuatnya merasa semakin tertekan dan semakin sulit untuk fokus pada pekerjaannya saat dia terus-menerus merasa gugup dan cemas setiap kali dia melangkah keluar dari gedung penerbitnya.  Maya segera menyadari bahwa ada sesuatu yang salah dengan dirinya dan dia harus berhati-hati dalam mengambil keputusan. Sesuatu yang tidak dia ketahui tentang dunia luar biasanya dan kehidupan orang-orang yang tinggal di dalamnya juga berada di luar jangkauannya. Selain itu, Maya juga mulai merasa takut dan terancam oleh makhluk-makhluk gaib yang berkeliaran di sekitar tempat dia tinggal dan berusaha untuk menguasai mereka. Untuk membantu mengatasi masalah-masalah yang dihadapi Maya, seorang penulisSeorang putri muda yang bernama Luna hidup dalam sebuah kerajaan besar yang dipimpin oleh raja dan ratu.\"\n",
    "\n",
    "reference_sentences = \"Seorang putri muda yang bernama Luna hidup dalam sebuah kerajaan besar yang dipimpin oleh raja dan ratu.Seorang putri muda yang bernama Luna hidup dalam sebuah kerajaan besar yang dipimpin oleh raja dan ratu. Luna adalah seorang gadis yang sangat menyukai petualangan. Suatu hari, dia mendengar kabar tentang sebuah hutan terlarang yang belum pernah dijelajahi oleh manusia sebelumnya. Dia merasa tertarik dan memutuskan untuk menjelajahi hutan itu sendirian.  Namun, ketika dia tiba di sana, sebuah badai menghadang di depan matanya dan memaksanya untuk bertahan hidup dengan cara yang paling aman dan terbaik yang bisa dia lakukan.  Seorang wanita muda bernama Maya baru saja lulus kuliah dan memulai pekerjaan barunya sebagai editor buku di sebuah penerbit terkenal. Maya sedang mencari penerbit yang tepat untuk menerbitkan bukunya, tetapi dia tidak tahu harus mulai dari mana. Hal ini menyebabkan dia merasa kesulitan untuk memulai bisnis barunya dan pada akhirnya membuatnya merasa semakin tertekan dan semakin sulit untuk fokus pada pekerjaannya saat dia terus-menerus merasa gugup dan cemas setiap kali dia melangkah keluar dari gedung penerbitnya.  Maya segera menyadari bahwa ada sesuatu yang salah dengan dirinya dan dia harus berhati-hati dalam mengambil keputusan. Sesuatu yang tidak dia ketahui tentang dunia luar biasanya dan kehidupan orang-orang yang tinggal di dalamnya juga berada di luar jangkauannya. Selain itu, Maya juga mulai merasa takut dan terancam oleh makhluk-makhluk gaib yang berkeliaran di sekitar tempat dia tinggal dan berusaha untuk menguasai mereka. Untuk membantu mengatasi masalah-masalah yang dihadapi Maya, seorang penulisSeorang putri muda yang bernama Luna hidup dalam sebuah kerajaan besar yang dipimpin oleh raja dan ratu.\"\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "# rouge_scores = []\n",
    "# for ref_sentence in reference_sentences:\n",
    "#     scores = rouge.get_scores(gpt2_output, ref_sentence)\n",
    "#     rouge_scores.append(scores[0])\n",
    "\n",
    "scores = rouge.get_scores(gpt2_output, reference_sentences, avg=True)\n",
    "rouge_score = scores\n",
    "print(\"ROUGE Score:\", rouge_score)\n",
    "\n",
    "# # Find the highest ROUGE scores\n",
    "# highest_rouge_1 = max(rouge_scores, key=lambda x: x['rouge-1']['f'])\n",
    "# highest_rouge_2 = max(rouge_scores, key=lambda x: x['rouge-2']['f'])\n",
    "# highest_rouge_l = max(rouge_scores, key=lambda x: x['rouge-l']['f'])\n",
    "\n",
    "# # Print the highest ROUGE scores\n",
    "# print(\"Highest ROUGE-1 score:\", highest_rouge_1['rouge-1']['f'])\n",
    "# print(\"Highest ROUGE-1 score:\", highest_rouge_1['rouge-1']['p'])\n",
    "# print(\"Highest ROUGE-1 score:\", highest_rouge_1['rouge-1']['r'])\n",
    "# print(\"Highest ROUGE-2 score:\", highest_rouge_2['rouge-2']['f'])\n",
    "# print(\"Highest ROUGE-2 score:\", highest_rouge_2['rouge-2']['p'])\n",
    "# print(\"Highest ROUGE-2 score:\", highest_rouge_2['rouge-2']['r'])\n",
    "# print(\"Highest ROUGE-L score:\", highest_rouge_l['rouge-l']['f'])\n",
    "# print(\"Highest ROUGE-L score:\", highest_rouge_l['rouge-l']['p'])\n",
    "# print(\"Highest ROUGE-L score:\", highest_rouge_l['rouge-l']['r'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hyp_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrouge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FilesRouge\n\u001b[0;32m      3\u001b[0m files_rouge \u001b[38;5;241m=\u001b[39m FilesRouge()\n\u001b[1;32m----> 4\u001b[0m scores \u001b[38;5;241m=\u001b[39m files_rouge\u001b[38;5;241m.\u001b[39mget_scores(\u001b[43mhyp_path\u001b[49m, ref_path)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# or\u001b[39;00m\n\u001b[0;32m      6\u001b[0m scores \u001b[38;5;241m=\u001b[39m files_rouge\u001b[38;5;241m.\u001b[39mget_scores(hyp_path, ref_path, avg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hyp_path' is not defined"
     ]
    }
   ],
   "source": [
    "from rouge import FilesRouge\n",
    "\n",
    "files_rouge = FilesRouge()\n",
    "scores = files_rouge.get_scores(hyp_path, ref_path)\n",
    "# or\n",
    "scores = files_rouge.get_scores(hyp_path, ref_path, avg=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I loved reading the Hunger Games\n",
      "ROUGE Score: {'rouge-1': {'r': 1.0, 'p': 0.8571428571428571, 'f': 0.9230769181065088}, 'rouge-2': {'r': 0.8, 'p': 0.6666666666666666, 'f': 0.7272727223140496}, 'rouge-l': {'r': 1.0, 'p': 0.8571428571428571, 'f': 0.9230769181065088}}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from rouge import Rouge\n",
    "\n",
    "gpt2_output = \"I really loved reading the Hunger Games\"\n",
    "reference_sentence = \"I loved reading the Hunger Games\"\n",
    "\n",
    "gpt2_tokens = nltk.word_tokenize(gpt2_output)\n",
    "reference_tokens = nltk.word_tokenize(reference_sentence)\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "# Convert tokens into strings\n",
    "gpt2_text = ' '.join(gpt2_tokens)\n",
    "reference_text = ' '.join(reference_tokens)\n",
    "\n",
    "print (reference_text)\n",
    "\n",
    "scores = rouge.get_scores(gpt2_text, reference_text)\n",
    "rouge_score = scores[0]\n",
    "\n",
    "print(\"ROUGE Score:\", rouge_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge-score in c:\\users\\asus\\anaconda3\\envs\\caret_testing\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in c:\\users\\asus\\anaconda3\\envs\\caret_testing\\lib\\site-packages (from rouge-score) (2.0.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\asus\\anaconda3\\envs\\caret_testing\\lib\\site-packages (from rouge-score) (3.8.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\asus\\anaconda3\\envs\\caret_testing\\lib\\site-packages (from rouge-score) (1.23.5)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\asus\\anaconda3\\envs\\caret_testing\\lib\\site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\asus\\anaconda3\\envs\\caret_testing\\lib\\site-packages (from nltk->rouge-score) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\asus\\anaconda3\\envs\\caret_testing\\lib\\site-packages (from nltk->rouge-score) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\asus\\anaconda3\\envs\\caret_testing\\lib\\site-packages (from nltk->rouge-score) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\anaconda3\\envs\\caret_testing\\lib\\site-packages (from nltk->rouge-score) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\envs\\caret_testing\\lib\\site-packages (from click->nltk->rouge-score) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rouge_metric'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrouge_metric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PerlRouge\n\u001b[0;32m      3\u001b[0m rouge \u001b[38;5;241m=\u001b[39m PerlRouge(rouge_n_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, rouge_l\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, rouge_w\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      4\u001b[0m     rouge_w_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.2\u001b[39m, rouge_s\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, rouge_su\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, skip_gap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load summary results and evaluate\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'rouge_metric'"
     ]
    }
   ],
   "source": [
    "from rouge_metric import PerlRouge\n",
    "\n",
    "rouge = PerlRouge(rouge_n_max=3, rouge_l=True, rouge_w=True,\n",
    "    rouge_w_weight=1.2, rouge_s=True, rouge_su=True, skip_gap=4)\n",
    "\n",
    "# Load summary results and evaluate\n",
    "hypotheses = [\n",
    "    'how are you\\ni am fine',                       # document 1: hypothesis\n",
    "    'it is fine today\\nwe won the football game',   # document 2: hypothesis\n",
    "]\n",
    "references = [[\n",
    "    'how do you do\\nfine thanks',   # document 1: reference 1\n",
    "    'how old are you\\ni am three',  # document 1: reference 2\n",
    "], [\n",
    "    'it is sunny today\\nlet us go for a walk',  # document 2: reference 1\n",
    "    'it is a terrible day\\nwe lost the game',   # document 2: reference 2\n",
    "]]\n",
    "\n",
    "scores = rouge.evaluate(hypotheses, references)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "scores = scorer.score('The quick brown fox jumps over the lazy dog',\n",
    "                      'The quick brown dog jumps on the log.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Reference:\n",
      "<|startoftext|> Laura sedang berjalan pulang dari sekolah. <|sentence1to2|> Dia mendengar suara di belakangnya. <|sentence2to3|> Dia melihat ke belakang. <|sentence3to4|> Dia melihat bukan apa -apa. <|sentence4to5|> Laura diam -diam diikuti oleh seorang pria misterius. <|endoftext|>\n",
      "\n",
      "ROUGE Scores:\n",
      "ROUGE-1 Precision: 0.2962962962962963\n",
      "ROUGE-1 Recall: 0.17777777777777778\n",
      "ROUGE-1 F1 Score: 0.2222222222222222\n",
      "ROUGE-2 Precision: 0.038461538461538464\n",
      "ROUGE-2 Recall: 0.022727272727272728\n",
      "ROUGE-2 F1 Score: 0.028571428571428574\n",
      "ROUGE-L Precision: 0.25925925925925924\n",
      "ROUGE-L Recall: 0.15555555555555556\n",
      "ROUGE-L F1 Score: 0.19444444444444445\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "file_path = \"train_translated_output.txt\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    array_train_dataset = f.read().splitlines()\n",
    "    \n",
    "# Example prediction\n",
    "prediction = \"<|startoftext|> Suatu hari Scott sedang berjalan pulang dari sekolah. <|sentence1to2|> Dia melihat seekor anjing besar di halaman belakang rumahnya. <|sentence2to3|> Anjing itu berlari ke arahnya dan menggigitnya. <|sentence3to4|> Seorang pria datang ke Scott lalu membunuh anjing itu. <|sentence4to5|> Scott merasa kesakitan setelah kakinya digigit. <|endoftext|>\"\n",
    "\n",
    "# Example references\n",
    "references = array_train_dataset\n",
    "\n",
    "# Initialize the ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL','rouge1','rouge2'], use_stemmer=True)\n",
    "\n",
    "# Initialize variables to store the best reference and corresponding ROUGE scores\n",
    "best_reference = \"\"\n",
    "max_rouge_score = 0.0\n",
    "\n",
    "# Compute ROUGE scores for each reference and find the best-matching reference\n",
    "for reference in references:\n",
    "    # Compute ROUGE scores\n",
    "    rouge_scores = scorer.score(prediction, reference)\n",
    "\n",
    "    # Use ROUGE-L F1 score for comparison\n",
    "    rouge_l_score = rouge_scores['rougeL'].fmeasure\n",
    "\n",
    "    # Update the best reference if the current reference has a higher score\n",
    "    if rouge_l_score > max_rouge_score:\n",
    "        max_rouge_score = rouge_l_score\n",
    "        best_reference = reference\n",
    "\n",
    "# Print the best reference and corresponding ROUGE scores\n",
    "print(\"Best Reference:\")\n",
    "print(best_reference)\n",
    "print(\"\\nROUGE Scores:\")\n",
    "\n",
    "print(f\"ROUGE-1 Precision: {rouge_scores['rouge1'].precision}\")\n",
    "print(f\"ROUGE-1 Recall: {rouge_scores['rouge1'].recall}\")\n",
    "print(f\"ROUGE-1 F1 Score: {rouge_scores['rouge1'].fmeasure}\")\n",
    "\n",
    "print(f\"ROUGE-2 Precision: {rouge_scores['rouge2'].precision}\")\n",
    "print(f\"ROUGE-2 Recall: {rouge_scores['rouge2'].recall}\")\n",
    "print(f\"ROUGE-2 F1 Score: {rouge_scores['rouge2'].fmeasure}\")\n",
    "\n",
    "print(f\"ROUGE-L Precision: {rouge_scores['rougeL'].precision}\")\n",
    "print(f\"ROUGE-L Recall: {rouge_scores['rougeL'].recall}\")\n",
    "print(f\"ROUGE-L F1 Score: {rouge_scores['rougeL'].fmeasure}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative 2-gram: 0.325669\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n",
      "BLEU score: 8.512570894324454e-155\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "file_path = \"train_translated_output.txt\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    array_train_dataset = f.read().splitlines()\n",
    "\n",
    "gpt2_output = \"The cat sat on the mat and enjoyed a peaceful afternoon.\"\n",
    "\n",
    "reference_sentences = [\"The feline perched on the rug and savored a calm afternoon.\"]\n",
    "\n",
    "gpt2_tokens = nltk.word_tokenize(gpt2_output)\n",
    "reference_tokens = [nltk.word_tokenize(sentence) for sentence in reference_sentences]\n",
    "\n",
    "bleu_score = sentence_bleu(reference_tokens, gpt2_tokens)\n",
    "\n",
    "print('Cumulative 2-gram: %f' % sentence_bleu(reference_tokens, gpt2_tokens, weights=(0.5, 0.5, 0, 0)))\n",
    "print('Cumulative 3-gram: %f' % sentence_bleu(reference_tokens, gpt2_tokens, weights=(0.33, 0.33, 0.33, 0)))\n",
    "print('Cumulative 4-gram: %f' % sentence_bleu(reference_tokens, gpt2_tokens, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "print(\"BLEU score:\", bleu_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ROUGE Scores:\n",
      "rouge1 Precision: 0.2\n",
      "rouge1 Recall: 0.2647058823529412\n",
      "rouge1 F1 Score: 0.2278481012658228\n",
      "rouge2 Precision: 0.022727272727272728\n",
      "rouge2 Recall: 0.030303030303030304\n",
      "rouge2 F1 Score: 0.025974025974025976\n",
      "rougeL Precision: 0.17777777777777778\n",
      "rougeL Recall: 0.23529411764705882\n",
      "rougeL F1 Score: 0.20253164556962028\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "file_path = \"train_translated_output.txt\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    array_train_dataset = f.read().splitlines()\n",
    "\n",
    "prediction = \"<|startoftext|> Saya perempuannya. <|sentence1to2|> Ketika kue mulai meleleh, dia melihat ke bawah. <|sentence2to3|> Dia pikir itu terlalu panas! <|sentence3to4|> Dia tidak bisa menunggu lebih lama lagi. <|sentence4to5|> Jadi dia memutuskan untuk mencoba resep baru.\"\n",
    "\n",
    "references = array_train_dataset\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "max_rouge_scores = {metric: 0.0 for metric in ['rouge1', 'rouge2', 'rougeL']}\n",
    "\n",
    "for reference in references:\n",
    "    rouge_scores = scorer.score(prediction, reference)\n",
    "\n",
    "    for metric in ['rouge1', 'rouge2', 'rougeL']:\n",
    "        if rouge_scores[metric].fmeasure > max_rouge_scores[metric]:\n",
    "            max_rouge_scores[metric] = rouge_scores[metric].fmeasure\n",
    "\n",
    "print(\"\\nROUGE Scores:\")\n",
    "for metric in ['rouge1', 'rouge2', 'rougeL']:\n",
    "    print(f\"{metric} Precision: {rouge_scores[metric].precision}\")\n",
    "    print(f\"{metric} Recall: {rouge_scores[metric].recall}\")\n",
    "    print(f\"{metric} F1 Score: {rouge_scores[metric].fmeasure}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best References:\n",
      "rouge1: <|startoftext|> Suatu hari Scott sedang berjalan pulang dari sekolah. <|sentence1to2|> Dia mendengar suara yang datang dari semak -semak. <|sentence2to3|> Dia pergi lebih dekat untuk memeriksanya. <|sentence3to4|> Itu adalah anak anjing kecil tanpa kerah. <|sentence4to5|> Scott memutuskan untuk membawa pulang anak anjing itu dan membuatnya sendiri. <|endoftext|>\n",
      "rouge1 Precision: 0.2962962962962963\n",
      "rouge1 Recall: 0.17777777777777778\n",
      "rouge1 F1 Score: 0.2222222222222222\n",
      "\n",
      "rouge2: <|startoftext|> Suatu hari Scott sedang berjalan pulang dari sekolah. <|sentence1to2|> Dia mendengar suara yang datang dari semak -semak. <|sentence2to3|> Dia pergi lebih dekat untuk memeriksanya. <|sentence3to4|> Itu adalah anak anjing kecil tanpa kerah. <|sentence4to5|> Scott memutuskan untuk membawa pulang anak anjing itu dan membuatnya sendiri. <|endoftext|>\n",
      "rouge2 Precision: 0.038461538461538464\n",
      "rouge2 Recall: 0.022727272727272728\n",
      "rouge2 F1 Score: 0.028571428571428574\n",
      "\n",
      "rougeL: <|startoftext|> Laura sedang berjalan pulang dari sekolah. <|sentence1to2|> Dia mendengar suara di belakangnya. <|sentence2to3|> Dia melihat ke belakang. <|sentence3to4|> Dia melihat bukan apa -apa. <|sentence4to5|> Laura diam -diam diikuti oleh seorang pria misterius. <|endoftext|>\n",
      "rougeL Precision: 0.25925925925925924\n",
      "rougeL Recall: 0.15555555555555556\n",
      "rougeL F1 Score: 0.19444444444444445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "file_path = \"train_translated_output.txt\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    array_train_dataset = f.read().splitlines()\n",
    "    \n",
    "prediction = \"<|startoftext|> Suatu hari Scott sedang berjalan pulang dari sekolah. <|sentence1to2|> Dia melihat seekor anjing besar di halaman belakang rumahnya. <|sentence2to3|> Anjing itu berlari ke arahnya dan menggigitnya. <|sentence3to4|> Seorang pria datang ke Scott lalu membunuh anjing itu. <|sentence4to5|> Scott merasa kesakitan setelah kakinya digigit. <|endoftext|>\"\n",
    "\n",
    "references = array_train_dataset\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "best_references = {metric: \"\" for metric in ['rouge1', 'rouge2', 'rougeL']}\n",
    "max_rouge_scores = {metric: 0.0 for metric in ['rouge1', 'rouge2', 'rougeL']}\n",
    "\n",
    "for reference in references:\n",
    "    rouge_scores = scorer.score(prediction, reference)\n",
    "\n",
    "    for metric in ['rouge1', 'rouge2', 'rougeL']:\n",
    "        if rouge_scores[metric].fmeasure > max_rouge_scores[metric]:\n",
    "            max_rouge_scores[metric] = rouge_scores[metric].fmeasure\n",
    "            best_references[metric] = reference\n",
    "\n",
    "print(\"Best References:\")\n",
    "for metric in ['rouge1', 'rouge2', 'rougeL']:\n",
    "    print(f\"{metric}: {best_references[metric]}\")\n",
    "    print(f\"{metric} Precision: {rouge_scores[metric].precision}\")\n",
    "    print(f\"{metric} Recall: {rouge_scores[metric].recall}\")\n",
    "    print(f\"{metric} F1 Score: {rouge_scores[metric].fmeasure}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asus\\anaconda3\\envs\\Web-Transformers-Nvidia\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best BLEU Reference:\n",
      "<|startoftext|> Suatu hari Scott sedang berjalan pulang dari sekolah. <|sentence1to2|> Dia mendengar suara yang datang dari semak -semak. <|sentence2to3|> Dia pergi lebih dekat untuk memeriksanya. <|sentence3to4|> Itu adalah anak anjing kecil tanpa kerah. <|sentence4to5|> Scott memutuskan untuk membawa pulang anak anjing itu dan membuatnya sendiri. <|endoftext|>\n",
      "\n",
      "BLEU Score: 0.42277523300671854\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "file_path = \"train_translated_output.txt\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    array_train_dataset = f.read().splitlines()\n",
    "\n",
    "# Example prediction\n",
    "prediction = \"<|startoftext|> Suatu hari Scott sedang berjalan pulang dari sekolah. <|sentence1to2|> Dia melihat seekor anjing besar di halaman belakang rumahnya. <|sentence2to3|> Anjing itu berlari ke arahnya dan menggigitnya. <|sentence3to4|> Seorang pria datang ke Scott lalu membunuh anjing itu. <|sentence4to5|> Scott merasa kesakitan setelah kakinya digigit. <|endoftext|>\"\n",
    "\n",
    "# Example references\n",
    "references = \"array_train_dataset\"\n",
    "\n",
    "# Initialize variables to store the best reference and corresponding BLEU score\n",
    "best_reference = \"\"\n",
    "max_bleu_score = 0.0\n",
    "\n",
    "# Compute BLEU scores for each reference and find the best-matching reference\n",
    "for reference in references:\n",
    "    # Tokenize the prediction and reference\n",
    "    prediction_tokens = nltk.word_tokenize(prediction)\n",
    "    reference_tokens = nltk.word_tokenize(reference)\n",
    "\n",
    "    # Compute BLEU score\n",
    "    bleu_score = sentence_bleu([reference_tokens], prediction_tokens)\n",
    "\n",
    "    # Update the best reference if the current reference has a higher BLEU score\n",
    "    if bleu_score > max_bleu_score:\n",
    "        max_bleu_score = bleu_score\n",
    "        best_reference = reference\n",
    "\n",
    "# Print the best reference and corresponding BLEU score\n",
    "print(\"Best BLEU Reference:\")\n",
    "print(best_reference)\n",
    "print(\"\\nBLEU Score:\", max_bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative 2-gram: 0.574684\n",
      "Cumulative 3-gram: 0.509943\n",
      "Cumulative 4-gram: 0.444460\n",
      "BLEU score: 0.444459729828027\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "file_path = \"train_translated_output.txt\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    array_train_dataset = f.read().splitlines()\n",
    "\n",
    "gpt2_output = \"<|startoftext|> Suatu hari Scott sedang berjalan pulang dari sekolah. <|sentence1to2|> Dia melihat seekor anjing besar di halaman belakang rumahnya. <|sentence2to3|> Anjing itu berlari ke arahnya dan menggigitnya. <|sentence3to4|> Seorang pria datang ke Scott lalu membunuh anjing itu. <|sentence4to5|> Scott merasa kesakitan setelah kakinya digigit. <|endoftext|>\"\n",
    "\n",
    "reference_sentences = [\"<|startoftext|> Laura sedang berjalan pulang dari sekolah. <|sentence1to2|> Dia mendengar suara di belakangnya. <|sentence2to3|> Dia melihat ke belakang. <|sentence3to4|> Dia melihat bukan apa -apa. <|sentence4to5|> Laura diam -diam diikuti oleh seorang pria misterius. <|endoftext|>\", \"<|startoftext|> Suatu hari Scott sedang berjalan pulang dari sekolah. <|sentence1to2|> Dia mendengar suara yang datang dari semak -semak. <|sentence2to3|> Dia pergi lebih dekat untuk memeriksanya. <|sentence3to4|> Itu adalah anak anjing kecil tanpa kerah. <|sentence4to5|> Scott memutuskan untuk membawa pulang anak anjing itu dan membuatnya sendiri. <|endoftext|>\"]\n",
    "\n",
    "gpt2_tokens = nltk.word_tokenize(gpt2_output)\n",
    "reference_tokens = [nltk.word_tokenize(sentence) for sentence in reference_sentences]\n",
    "\n",
    "bleu_score = sentence_bleu(reference_tokens, gpt2_tokens)\n",
    "\n",
    "\n",
    "print('Cumulative 2-gram: %f' % sentence_bleu(reference_tokens, gpt2_tokens, weights=(0.5, 0.5, 0, 0)))\n",
    "print('Cumulative 3-gram: %f' % sentence_bleu(reference_tokens, gpt2_tokens, weights=(0.33, 0.33, 0.33, 0)))\n",
    "print('Cumulative 4-gram: %f' % sentence_bleu(reference_tokens, gpt2_tokens, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "print(\"BLEU score:\", bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best References:\n",
      "rouge1: ['<|startoftext|> Suatu hari Scott sedang berjalan pulang dari sekolah. <|sentence1to2|> Dia mendengar suara yang datang dari semak -semak. <|sentence2to3|> Dia pergi lebih dekat untuk memeriksanya. <|sentence3to4|> Itu adalah anak anjing kecil tanpa kerah. <|sentence4to5|> Scott memutuskan untuk membawa pulang anak anjing itu dan membuatnya sendiri. <|endoftext|>', '<|startoftext|> Laura sedang berjalan pulang dari sekolah. <|sentence1to2|> Dia mendengar suara di belakangnya. <|sentence2to3|> Dia melihat ke belakang. <|sentence3to4|> Dia melihat bukan apa -apa. <|sentence4to5|> Laura diam -diam diikuti oleh seorang pria misterius. <|endoftext|>', '<|startoftext|> Shelly membiarkan anjingnya keluar di halaman belakang untuk bermain. <|sentence1to2|> Beberapa saat kemudian dia melihat seekor groundhog mengintip dari bawah gudang. <|sentence2to3|> Dia memanggil anjing -anjing itu kembali ke dalam sebelum mereka melihat makhluk itu. <|sentence3to4|> Begitu masuk, anjing melihat groundhog dan mulai menggonggong. <|sentence4to5|> Groundhog kecil berlari di bawah pagar ke halaman tetangga. <|endoftext|>', '<|startoftext|> Bill sedang berjalan pulang dari kantor. <|sentence1to2|> Dan seorang pria tunawisma memintanya uang. <|sentence2to3|> Bill memberi pria itu beberapa dolar. <|sentence3to4|> Tapi dia tidak nyaman dengan seberapa dekat pria itu datang kepadanya. <|sentence4to5|> Bill mulai merasa sakit setelah dia tiba di rumah. <|endoftext|>']\n",
      "rouge1 Precision: 0.07407407407407407\n",
      "rouge1 Recall: 0.05128205128205128\n",
      "rouge1 F1 Score: 0.060606060606060615\n",
      "\n",
      "rouge2: ['<|startoftext|> Laura sedang berjalan pulang dari sekolah. <|sentence1to2|> Dia mendengar suara di belakangnya. <|sentence2to3|> Dia melihat ke belakang. <|sentence3to4|> Dia melihat bukan apa -apa. <|sentence4to5|> Laura diam -diam diikuti oleh seorang pria misterius. <|endoftext|>', '<|startoftext|> Aaron takut ular. <|sentence1to2|> Ketika dia pulang dari sekolah, dia melihat seekor ular di halaman. <|sentence2to3|> Dia mulai gemetar dan menangis. <|sentence3to4|> Ayahnya pulang dan menyingkirkan ular itu. <|sentence4to5|> Butuh beberapa saat bagi Aaron untuk tenang. <|endoftext|>', '<|startoftext|> Tina sedang berjalan di dekat rel kereta api. <|sentence1to2|> Dia melihat seorang pria tunawisma goyah di trek. <|sentence2to3|> Dia jelas mabuk, kereta api yang datang ke kejauhan. <|sentence3to4|> Dia berlari ke arahnya, berteriak dan melambaikan tangannya. <|sentence4to5|> Dengan bersendawa, dia tersandung dari trek dan pingsan di parit. <|endoftext|>', '<|startoftext|> Suatu hari Scott sedang berjalan pulang dari sekolah. <|sentence1to2|> Dia mendengar suara yang datang dari semak -semak. <|sentence2to3|> Dia pergi lebih dekat untuk memeriksanya. <|sentence3to4|> Itu adalah anak anjing kecil tanpa kerah. <|sentence4to5|> Scott memutuskan untuk membawa pulang anak anjing itu dan membuatnya sendiri. <|endoftext|>']\n",
      "rouge2 Precision: 0.0\n",
      "rouge2 Recall: 0.0\n",
      "rouge2 F1 Score: 0.0\n",
      "\n",
      "rougeL: ['<|startoftext|> Suatu hari Scott sedang berjalan pulang dari sekolah. <|sentence1to2|> Dia mendengar suara yang datang dari semak -semak. <|sentence2to3|> Dia pergi lebih dekat untuk memeriksanya. <|sentence3to4|> Itu adalah anak anjing kecil tanpa kerah. <|sentence4to5|> Scott memutuskan untuk membawa pulang anak anjing itu dan membuatnya sendiri. <|endoftext|>', '<|startoftext|> Tim membeli anjing baru. <|sentence1to2|> Suatu hari dia berjalan anjing di hutan besar. <|sentence2to3|> Anjing itu berlari ke semak -semak. <|sentence3to4|> Tim mencari satu jam mencoba menemukan anjingnya. <|sentence4to5|> Anjing itu akhirnya muncul membawa tongkat terbesar yang pernah ada. <|endoftext|>', '<|startoftext|> Billy berjalan di jalan suatu hari. <|sentence1to2|> Dia melihat seekor kucing terjebak di pohon. <|sentence2to3|> Dia mencoba menyebutnya. <|sentence3to4|> Kucing itu takut dan tidak datang kepadanya. <|sentence4to5|> Dia memanjat pohon itu dan menyelamatkan kucing itu sendiri. <|endoftext|>', '<|startoftext|> Laura sedang berjalan pulang dari sekolah. <|sentence1to2|> Dia mendengar suara di belakangnya. <|sentence2to3|> Dia melihat ke belakang. <|sentence3to4|> Dia melihat bukan apa -apa. <|sentence4to5|> Laura diam -diam diikuti oleh seorang pria misterius. <|endoftext|>']\n",
      "rougeL Precision: 0.07407407407407407\n",
      "rougeL Recall: 0.05128205128205128\n",
      "rougeL F1 Score: 0.060606060606060615\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "file_path = \"train_translated_output.txt\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    array_train_dataset = f.read().splitlines()\n",
    "\n",
    "prediction = \" Suatu hari Scott sedang berjalan pulang dari sekolah. Dia melihat seekor anjing besar di halaman belakang rumahnya. Anjing itu berlari ke arahnya dan menggigitnya. Seorang pria datang ke Scott lalu membunuh anjing itu. Scott merasa kesakitan setelah kakinya digigit. \"\n",
    "\n",
    "references = array_train_dataset\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Initialize variables to store the best references and corresponding ROUGE scores\n",
    "best_references = {metric: [\"\", \"\", \"\", \"\"] for metric in ['rouge1', 'rouge2', 'rougeL']}\n",
    "max_rouge_scores = {metric: [0.0, 0.0, 0.0, 0.0] for metric in ['rouge1', 'rouge2', 'rougeL']}\n",
    "\n",
    "for reference in references:\n",
    "    rouge_scores = scorer.score(prediction, reference)\n",
    "\n",
    "    for metric in ['rouge1', 'rouge2', 'rougeL']:\n",
    "        # Check if the list is empty\n",
    "        if not best_references[metric][0]:\n",
    "            best_references[metric] = [reference] * 4\n",
    "            max_rouge_scores[metric] = [rouge_scores[metric].fmeasure] * 4\n",
    "        else:\n",
    "            # Find the minimum score among the top 4 references for each metric\n",
    "            min_score = min(max_rouge_scores[metric])\n",
    "            \n",
    "            if rouge_scores[metric].fmeasure > min_score:\n",
    "                # Replace the reference with the minimum score\n",
    "                min_index = max_rouge_scores[metric].index(min_score)\n",
    "                max_rouge_scores[metric][min_index] = rouge_scores[metric].fmeasure\n",
    "                best_references[metric][min_index] = reference\n",
    "\n",
    "print(\"Best References:\")\n",
    "for metric in ['rouge1', 'rouge2', 'rougeL']:\n",
    "    print(f\"{metric}: {best_references[metric]}\")\n",
    "    print(f\"{metric} Precision: {rouge_scores[metric].precision}\")\n",
    "    print(f\"{metric} Recall: {rouge_scores[metric].recall}\")\n",
    "    print(f\"{metric} F1 Score: {rouge_scores[metric].fmeasure}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 8.90735602648238e-232\n",
      "\n",
      "Best-matching reference:\n",
      "<\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asus\\anaconda3\\envs\\caret_testing\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\asus\\anaconda3\\envs\\caret_testing\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "file_path = \"train_translated_output.txt\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    array_train_dataset = f.read().splitlines()\n",
    "\n",
    "gpt2_output = \"<|startoftext|> anya ingin belajar cara menulis. <|sentence1to2|> Dia pergi ke toko buku dan membeli beberapa novel yang bagus. <|sentence2to3|> Ketika dia sampai di rumah, dia membaca semua bukunya. <|sentence3to4|> Setelah selesai membacanya, dia menyadari bahwa dia tidak bisa menulis dengan baik. <|sentence4to5|> Dia memutuskan untuk mencoba lagi suatu hari nanti. <|endoftext|>\"\n",
    "\n",
    "reference_sentences = \"<|startoftext|> Stan memutuskan dia membutuhkan lebih banyak peralatan olahraga. <|sentence1to2|> Dia pergi ke toko untuk menemukan sesuatu yang menyenangkan. <|sentence2to3|> Stan membeli baseball. <|sentence3to4|> Ketika dia sampai di rumah, dia meletakkan bisbol di mantelnya. <|sentence4to5|> Stan senang dia membeli beberapa peralatan olahraga. <|endoftext|>\"\n",
    "\n",
    "gpt2_tokens = nltk.word_tokenize(gpt2_output)\n",
    "reference_tokens = [nltk.word_tokenize(sentence) for sentence in reference_sentences]\n",
    "\n",
    "\n",
    "bleu_score = sentence_bleu(reference_tokens, gpt2_tokens)\n",
    "\n",
    "print(\"BLEU score:\", bleu_score)\n",
    "\n",
    "# best_reference_index = max(range(len(reference_sentences)), key=lambda i: sentence_bleu([reference_tokens[i]], gpt2_tokens))\n",
    "# best_reference = reference_sentences[best_reference_index]\n",
    "\n",
    "# print(\"\\nBest-matching reference:\")\n",
    "# print(best_reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 Precision: 0.5454545454545454\n",
      "rouge1 Recall: 0.5454545454545454\n",
      "rouge1 F1 Score: 0.5454545454545454\n",
      "\n",
      "rouge2 Precision: 0.3\n",
      "rouge2 Recall: 0.3\n",
      "rouge2 F1 Score: 0.3\n",
      "\n",
      "rougeL Precision: 0.5454545454545454\n",
      "rougeL Recall: 0.5454545454545454\n",
      "rougeL F1 Score: 0.5454545454545454\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "file_path = \"train_translated_output.txt\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    train_dataset = f.read()\n",
    "    \n",
    "prediction = \"The cat sat on the mat and enjoyed a peaceful afternoon.\"\n",
    "\n",
    "references = [\"The cat lounged on the mat and had a calm afternoon.\",\"A feline rested on the mat, savoring a tranquil afternoon.\", \"The cat relaxed on the mat, enjoying a serene afternoon.\", \"A domestic cat sat on the mat, savoring a peaceful afternoon.\", \"A kitty rested on the mat and had a quiet afternoon.\"]\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "best_references = {metric: \"\" for metric in ['rouge1', 'rouge2', 'rougeL']}\n",
    "max_rouge_scores = {metric: 0.0 for metric in ['rouge1', 'rouge2', 'rougeL']}\n",
    "\n",
    "for reference in references:\n",
    "    rouge_scores = scorer.score(prediction, reference)\n",
    "\n",
    "    for metric in ['rouge1', 'rouge2', 'rougeL']:\n",
    "        if rouge_scores[metric].fmeasure > max_rouge_scores[metric]:\n",
    "            max_rouge_scores[metric] = rouge_scores[metric].fmeasure\n",
    "\n",
    "for metric in ['rouge1', 'rouge2', 'rougeL']:\n",
    "    print(f\"{metric} Precision: {rouge_scores[metric].precision}\")\n",
    "    print(f\"{metric} Recall: {rouge_scores[metric].recall}\")\n",
    "    print(f\"{metric} F1 Score: {rouge_scores[metric].fmeasure}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative 2-gram: 0.866025\n",
      "Cumulative 3-gram: 0.768352\n",
      "Cumulative 4-gram: 0.668740\n",
      "BLEU score: 0.668740304976422\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "file_path = \"train_translated_output.txt\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    array_train_dataset = f.read().splitlines()\n",
    "\n",
    "gpt2_output = \"The cat sat on the mat and enjoyed a peaceful afternoon.\"\n",
    "\n",
    "reference_sentences = [\"The cat lounged on the mat and had a calm afternoon.\",\"A feline rested on the mat, savoring a tranquil afternoon.\", \"The cat relaxed on the mat, enjoying a serene afternoon.\", \"A domestic cat sat on the mat, savoring a peaceful afternoon.\", \"A kitty rested on the mat and had a quiet afternoon.\"]\n",
    "\n",
    "gpt2_tokens = nltk.word_tokenize(gpt2_output)\n",
    "reference_tokens = [nltk.word_tokenize(sentence) for sentence in reference_sentences]\n",
    "\n",
    "bleu_score = sentence_bleu(reference_tokens, gpt2_tokens)\n",
    "\n",
    "\n",
    "print('Cumulative 2-gram: %f' % sentence_bleu(reference_tokens, gpt2_tokens, weights=(0.5, 0.5, 0, 0)))\n",
    "print('Cumulative 3-gram: %f' % sentence_bleu(reference_tokens, gpt2_tokens, weights=(0.33, 0.33, 0.33, 0)))\n",
    "print('Cumulative 4-gram: %f' % sentence_bleu(reference_tokens, gpt2_tokens, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "print(\"BLEU score:\", bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative 2-gram: 0.715566\n",
      "Cumulative 3-gram: 0.661127\n",
      "Cumulative 4-gram: 0.598801\n",
      "BLEU score: 0.5988005945384479\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "file_path = \"train_translated_output.txt\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    array_train_dataset = f.read().splitlines()\n",
    "\n",
    "gpt2_output = \"<|startoftext|> Suatu hari Scott sedang berjalan pulang dari sekolah. <|sentence1to2|> Dia melihat seekor anjing besar di halaman belakang rumahnya. <|sentence2to3|> Anjing itu berlari ke arahnya dan menggigitnya. <|sentence3to4|> Seorang pria datang ke Scott lalu membunuh anjing itu. <|sentence4to5|> Scott merasa kesakitan setelah kakinya digigit. <|endoftext|>\"\n",
    "\n",
    "reference_sentences = [\"<|startoftext|> Laura sedang berjalan pulang dari sekolah. <|sentence1to2|> Dia mendengar suara di belakangnya. <|sentence2to3|> Dia melihat ke belakang. <|sentence3to4|> Dia melihat bukan apa -apa. <|sentence4to5|> Laura diam -diam diikuti oleh seorang pria misterius. <|endoftext|>\",\"<|startoftext|> Suatu hari Scott sedang berjalan pulang dari sekolah. <|sentence1to2|> Dia mendengar suara yang datang dari semak -semak. <|sentence2to3|> Dia pergi lebih dekat untuk memeriksanya. <|sentence3to4|> Itu adalah anak anjing kecil tanpa kerah. <|sentence4to5|> Scott memutuskan untuk membawa pulang anak anjing itu dan membuatnya sendiri. <|endoftext|>\",\"<|startoftext|> Allie mencintai anjing. <|sentence1to2|> Dia melihat seekor anjing di trotoar hari ini. <|sentence2to3|> Dia memutuskan untuk memeliharanya. <|sentence3to4|> Namun ketika dia melakukannya, itu menggigitnya. <|sentence4to5|> Allie berteriak kesakitan. <|endoftext|>\",\"Ketika saya berjalan di taman, seekor anjing berlari ke arah saya. <|sentence1to2|> Saya tidak mengenali anjing itu, jadi saya mencari pemiliknya. <|sentence2to3|> Anjing dan saya berjalan di sekitar taman untuk sementara waktu. <|sentence3to4|> Seorang pria muncul berteriak nama anjing itu. <|sentence4to5|> Anjing itu berlari kepadanya dan mereka bersatu kembali satu sama lain. <|endoftext|>\"]\n",
    "\n",
    "gpt2_tokens = nltk.word_tokenize(gpt2_output)\n",
    "reference_tokens = [nltk.word_tokenize(sentence) for sentence in reference_sentences]\n",
    "\n",
    "bleu_score = sentence_bleu(reference_tokens, gpt2_tokens)\n",
    "\n",
    "\n",
    "print('Cumulative 2-gram: %f' % sentence_bleu(reference_tokens, gpt2_tokens, weights=(0.5, 0.5, 0, 0)))\n",
    "print('Cumulative 3-gram: %f' % sentence_bleu(reference_tokens, gpt2_tokens, weights=(0.33, 0.33, 0.33, 0)))\n",
    "print('Cumulative 4-gram: %f' % sentence_bleu(reference_tokens, gpt2_tokens, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "print(\"BLEU score:\", bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 Precision: 0.33962264150943394\n",
      "rouge1 Recall: 0.4\n",
      "rouge1 F1 Score: 0.36734693877551017\n",
      "\n",
      "rouge2 Precision: 0.17307692307692307\n",
      "rouge2 Recall: 0.20454545454545456\n",
      "rouge2 F1 Score: 0.18750000000000003\n",
      "\n",
      "rougeL Precision: 0.24528301886792453\n",
      "rougeL Recall: 0.28888888888888886\n",
      "rougeL F1 Score: 0.26530612244897955\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "file_path = \"train_translated_output.txt\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    train_dataset = f.read()\n",
    "    \n",
    "gpt2_output = \"<|startoftext|> Suatu hari Scott sedang berjalan pulang dari sekolah. <|sentence1to2|> Dia melihat seekor anjing besar di halaman belakang rumahnya. <|sentence2to3|> Anjing itu berlari ke arahnya dan menggigitnya. <|sentence3to4|> Seorang pria datang ke Scott lalu membunuh anjing itu. <|sentence4to5|> Scott merasa kesakitan setelah kakinya digigit. <|endoftext|>\"\n",
    "\n",
    "reference_sentences = [\"<|startoftext|> Laura sedang berjalan pulang dari sekolah. <|sentence1to2|> Dia mendengar suara di belakangnya. <|sentence2to3|> Dia melihat ke belakang. <|sentence3to4|> Dia melihat bukan apa -apa. <|sentence4to5|> Laura diam -diam diikuti oleh seorang pria misterius. <|endoftext|>\",\"<|startoftext|> Suatu hari Scott sedang berjalan pulang dari sekolah. <|sentence1to2|> Dia mendengar suara yang datang dari semak -semak. <|sentence2to3|> Dia pergi lebih dekat untuk memeriksanya. <|sentence3to4|> Itu adalah anak anjing kecil tanpa kerah. <|sentence4to5|> Scott memutuskan untuk membawa pulang anak anjing itu dan membuatnya sendiri. <|endoftext|>\",\"<|startoftext|> Allie mencintai anjing. <|sentence1to2|> Dia melihat seekor anjing di trotoar hari ini. <|sentence2to3|> Dia memutuskan untuk memeliharanya. <|sentence3to4|> Namun ketika dia melakukannya, itu menggigitnya. <|sentence4to5|> Allie berteriak kesakitan. <|endoftext|>\",\"Ketika saya berjalan di taman, seekor anjing berlari ke arah saya. <|sentence1to2|> Saya tidak mengenali anjing itu, jadi saya mencari pemiliknya. <|sentence2to3|> Anjing dan saya berjalan di sekitar taman untuk sementara waktu. <|sentence3to4|> Seorang pria muncul berteriak nama anjing itu. <|sentence4to5|> Anjing itu berlari kepadanya dan mereka bersatu kembali satu sama lain. <|endoftext|>\"]\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "best_references = {metric: \"\" for metric in ['rouge1', 'rouge2', 'rougeL']}\n",
    "max_rouge_scores = {metric: 0.0 for metric in ['rouge1', 'rouge2', 'rougeL']}\n",
    "\n",
    "for reference in reference_sentences:\n",
    "    rouge_scores = scorer.score(gpt2_output, reference)\n",
    "\n",
    "    for metric in ['rouge1', 'rouge2', 'rougeL']:\n",
    "        if rouge_scores[metric].fmeasure > max_rouge_scores[metric]:\n",
    "            max_rouge_scores[metric] = rouge_scores[metric].fmeasure\n",
    "\n",
    "for metric in ['rouge1', 'rouge2', 'rougeL']:\n",
    "    print(f\"{metric} Precision: {rouge_scores[metric].precision}\")\n",
    "    print(f\"{metric} Recall: {rouge_scores[metric].recall}\")\n",
    "    print(f\"{metric} F1 Score: {rouge_scores[metric].fmeasure}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ROUGE Scores:\n",
      "rouge1 Precision: 0.2962962962962963\n",
      "rouge1 Recall: 0.16666666666666666\n",
      "rouge1 F1 Score: 0.21333333333333335\n",
      "rouge2 Precision: 0.038461538461538464\n",
      "rouge2 Recall: 0.02127659574468085\n",
      "rouge2 F1 Score: 0.0273972602739726\n",
      "rougeL Precision: 0.2222222222222222\n",
      "rougeL Recall: 0.125\n",
      "rougeL F1 Score: 0.16\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "file_path = \"train_translated_output.txt\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    testing_dataset = f.read().splitlines()\n",
    "    \n",
    "array_testing_dataset = testing_dataset\n",
    "\n",
    "prediction = \"<|startoftext|> Suatu hari Scott sedang berjalan pulang dari sekolah. <|sentence1to2|> Dia melihat seekor anjing besar di halaman belakang rumahnya. <|sentence2to3|> Anjing itu berlari ke arahnya dan menggigitnya. <|sentence3to4|> IPria itu memberi tahu Scott bahwa dia telah membunuh anjing itu. <|sentence4to5|> Scott tidak senang dengan apa yang terjadi padanya.\"\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "max_rouge_scores = {metric: 0.0 for metric in ['rouge1', 'rouge2', 'rougeL']}\n",
    "\n",
    "for reference in array_testing_dataset:\n",
    "    rouge_scores = scorer.score(prediction, reference)\n",
    "\n",
    "    for metric in ['rouge1', 'rouge2', 'rougeL']:\n",
    "        if rouge_scores[metric].fmeasure > max_rouge_scores[metric]:\n",
    "            max_rouge_scores[metric] = rouge_scores[metric].fmeasure\n",
    "\n",
    "print(\"\\nROUGE Scores:\")\n",
    "for metric in ['rouge1', 'rouge2', 'rougeL']:\n",
    "    print(f\"{metric} Precision: {rouge_scores[metric].precision}\")\n",
    "    print(f\"{metric} Recall: {rouge_scores[metric].recall}\")\n",
    "    print(f\"{metric} F1 Score: {rouge_scores[metric].fmeasure}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2_tokens:  ['The', 'cat', 'sat', 'on', 'the', 'mat', 'and', 'enjoyed', 'a', 'peaceful', 'afternoon', '.']\n",
      "reference_tokens:  ['<', '|startoftext|', '>', 'Tom', 'mengalami', 'temperamen', 'yang', 'sangat', 'pendek', '.', '<', '|sentence1to2|', '>', 'Suatu', 'hari', 'seorang', 'tamu', 'membuatnya', 'sangat', 'marah', '.', '<', '|sentence2to3|', '>', 'Dia', 'meninju', 'lubang', 'di', 'dinding', 'rumahnya', '.', '<', '|sentence3to4|', '>', 'Tamu', 'Tom', 'menjadi', 'takut', 'dan', 'pergi', 'dengan', 'cepat', '.', '<', '|sentence4to5|', '>', 'Tom', 'duduk', 'di', 'sofa', 'dipenuhi', 'dengan', 'penyesalan', 'tentang', 'tindakannya', '.', '<', '|endoftext|', '>']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "file_path = \"train_translated_output.txt\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    array_train_dataset = f.read().splitlines()\n",
    "\n",
    "gpt2_output = \"The cat sat on the mat and enjoyed a peaceful afternoon.\"\n",
    "\n",
    "reference_sentences = array_train_dataset\n",
    "\n",
    "gpt2_tokens = nltk.word_tokenize(gpt2_output)\n",
    "reference_tokens = [nltk.word_tokenize(sentence) for sentence in reference_sentences]\n",
    "\n",
    "print (\"gpt2_tokens: \", gpt2_tokens)\n",
    "print (\"reference_tokens: \", reference_tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '<|startoftext|> Loe sama gue ngontrak di desa. <|sentence1to2|> Kami tidak punya tempat tinggal yang baik. <|sentence2to3|> Suatu hari kami pergi ke kota untuk membeli rumah baru. <|sentence3to4|> Ketika kami sampai di sana, itu sangat bagus. <|sentence4to5|> Semua orang senang! '}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "output = \"[{'generated_text': '<|startoftext|> Loe sama gue ngontrak di desa. <|sentence1to2|> Kami tidak punya tempat tinggal yang baik. <|sentence2to3|> Suatu hari kami pergi ke kota untuk membeli rumah baru. <|sentence3to4|> Ketika kami sampai di sana, itu sangat bagus. <|sentence4to5|> Semua orang senang! '}]\"\n",
    "output_text_str = output.replace(\"'\", \"\\\"\")\n",
    "output_text = json.loads(output_text_str)\n",
    "print (output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nilai BLEU :\n",
    "\n",
    "0,8217977164300458\n",
    "\n",
    "Nilai ROUGE :\n",
    "\n",
    "ROUGE 1 F1 Score :0,26865671641791045\n",
    "\n",
    "ROUGE 2 F1 Score :0,0923076923076923\n",
    "\n",
    "ROUGE L F1 Score :0,23880597014925375\n",
    "Nilai BLEU :\n",
    "\n",
    "0,8471097694371866\n",
    "\n",
    "Nilai ROUGE :\n",
    "\n",
    "ROUGE 1 F1 Score :0,22857142857142856\n",
    "\n",
    "ROUGE 2 F1 Score :0,029411764705882353\n",
    "\n",
    "ROUGE L F1 Score :0,22857142857142856\n",
    "Nilai BLEU :\n",
    "\n",
    "0,8194076197146649\n",
    "\n",
    "Nilai ROUGE :\n",
    "\n",
    "ROUGE 1 F1 Score :0,2\n",
    "\n",
    "ROUGE 2 F1 Score :0,029411764705882353\n",
    "\n",
    "ROUGE L F1 Score :0,17142857142857143\n",
    "Nilai BLEU :\n",
    "\n",
    "0,8233337305089042\n",
    "\n",
    "Nilai ROUGE :\n",
    "\n",
    "ROUGE 1 F1 Score :0,26865671641791045\n",
    "\n",
    "ROUGE 2 F1 Score :0,06153846153846155\n",
    "\n",
    "ROUGE L F1 Score :0,1791044776119403\n",
    "Nilai BLEU :\n",
    "\n",
    "0,7553350681601495\n",
    "\n",
    "Nilai ROUGE :\n",
    "\n",
    "ROUGE 1 F1 Score :0,22499999999999998\n",
    "\n",
    "ROUGE 2 F1 Score :0,025641025641025644\n",
    "\n",
    "ROUGE L F1 Score :0,2\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ROUGE Scores:\n",
    "rouge1 Precision: 0.2962962962962963\n",
    "rouge1 Recall: 0.16666666666666666\n",
    "rouge1 F1 Score: 0.21333333333333335\n",
    "rouge2 Precision: 0.038461538461538464\n",
    "rouge2 Recall: 0.02127659574468085\n",
    "rouge2 F1 Score: 0.0273972602739726\n",
    "rougeL Precision: 0.2222222222222222\n",
    "rougeL Recall: 0.125\n",
    "rougeL F1 Score: 0.16\n",
    "gpt2_tokens:  ['The', 'cat', 'sat', 'on', 'the', 'mat', 'and', 'enjoyed', 'a', 'peaceful', 'afternoon', '.']\n",
    "reference_tokens:  ['<', '|startoftext|', '>', 'Tom', 'mengalami', 'temperamen', 'yang', 'sangat', 'pendek', '.', '<', '|sentence1to2|', '>', 'Suatu', 'hari', 'seorang', 'tamu', 'membuatnya', 'sangat', 'marah', '.', '<', '|sentence2to3|', '>', 'Dia', 'meninju', 'lubang', 'di', 'dinding', 'rumahnya', '.', '<', '|sentence3to4|', '>', 'Tamu', 'Tom', 'menjadi', 'takut', 'dan', 'pergi', 'dengan', 'cepat', '.', '<', '|sentence4to5|', '>', 'Tom', 'duduk', 'di', 'sofa', 'dipenuhi', 'dengan', 'penyesalan', 'tentang', 'tindakannya', '.', '<', '|endoftext|', '>']\n",
    "[{'generated_text': '<|startoftext|> Loe sama gue ngontrak di desa. <|sentence1to2|> Kami tidak punya tempat tinggal yang baik. <|sentence2to3|> Suatu hari kami pergi ke kota untuk membeli rumah baru. <|sentence3to4|> Ketika kami sampai di sana, itu sangat bagus. <|sentence4to5|> Semua orang senang! '}]\n",
    "rouge1:\n",
    "[Score(precision=0.8571428571428571, recall=1.0, fmeasure=0.923076923076923), Score(precision=0.7142857142857143, recall=0.8333333333333334, fmeasure=0.7692307692307692)]\n",
    "rouge2:\n",
    "[Score(precision=0.6666666666666666, recall=0.8, fmeasure=0.7272727272727272), Score(precision=0.3333333333333333, recall=0.4, fmeasure=0.3636363636363636)]\n",
    "rougeL:\n",
    "[Score(precision=0.8571428571428571, recall=1.0, fmeasure=0.923076923076923), Score(precision=0.5714285714285714, recall=0.6666666666666666, fmeasure=0.6153846153846153)]\n",
    "---------------------------------------------------------------------------\n",
    "KeyboardInterrupt                         Traceback (most recent call last)\n",
    "Cell In[7], line 13\n",
    "     10 reference_sentences = array_train_dataset\n",
    "     12 gpt2_tokens = nltk.word_tokenize(gpt2_output)\n",
    "---> 13 reference_tokens = [nltk.word_tokenize(sentence) for sentence in reference_sentences]\n",
    "     15 # Calculate BLEU score with detailed n-gram precision\n",
    "     16 bleu_score, individual_ngram_precisions = sentence_bleu(reference_tokens, gpt2_tokens, smoothing_function=SmoothingFunction().method1, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "\n",
    "Cell In[7], line 13\n",
    "     10 reference_sentences = array_train_dataset\n",
    "     12 gpt2_tokens = nltk.word_tokenize(gpt2_output)\n",
    "---> 13 reference_tokens = [nltk.word_tokenize(sentence) for sentence in reference_sentences]\n",
    "     15 # Calculate BLEU score with detailed n-gram precision\n",
    "     16 bleu_score, individual_ngram_precisions = sentence_bleu(reference_tokens, gpt2_tokens, smoothing_function=SmoothingFunction().method1, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "\n",
    "File c:\\Users\\asus\\anaconda3\\envs\\Web-Transformers-Nvidia\\lib\\site-packages\\nltk\\tokenize\\__init__.py:130, in word_tokenize(text, language, preserve_line)\n",
    "    115 \"\"\"\n",
    "    116 Return a tokenized copy of *text*,\n",
    "    117 using NLTK's recommended word tokenizer\n",
    "   (...)\n",
    "    127 :type preserve_line: bool\n",
    "    128 \"\"\"\n",
    "    129 sentences = [text] if preserve_line else sent_tokenize(text, language)\n",
    "--> 130 return [\n",
    "...\n",
    "-> 1073         literals[index] = g(group) or empty\n",
    "   1074 except IndexError:\n",
    "   1075     raise error(\"invalid group reference %d\" % index)\n",
    "\n",
    "KeyboardInterrupt: \n",
    "Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...\n",
    "C:\\Users\\asus\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
    "  from .autonotebook import tqdm as notebook_tqdm\n",
    "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_24100\\4144741119.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
    "  metric = load_metric(\"bleu\")\n",
    "Downloading builder script: 6.06kB [00:00, 3.06MB/s]                   \n",
    "Downloading extra modules: 4.07kB [00:00, 4.07MB/s]                   \n",
    "---------------------------------------------------------------------------\n",
    "ValueError                                Traceback (most recent call last)\n",
    "d:\\Text generation using GPT-2 (new dataset)\\eval.ipynb Cell 2 line 3\n",
    "      1 from datasets import load_metric\n",
    "      2 metric = load_metric(\"bleu\")\n",
    "----> 3 metric.compute(predictions=[\"hello world\"], references=[[\"hello world\", \"hello\"]])\n",
    "\n",
    "File c:\\Users\\asus\\anaconda3\\envs\\web-Pipeline\\lib\\site-packages\\datasets\\metric.py:442, in Metric.compute(self, predictions, references, **kwargs)\n",
    "    439 compute_kwargs = {k: kwargs[k] for k in kwargs if k not in self.features}\n",
    "    441 if any(v is not None for v in inputs.values()):\n",
    "--> 442     self.add_batch(**inputs)\n",
    "    443 self._finalize()\n",
    "    445 self.cache_file_name = None\n",
    "\n",
    "File c:\\Users\\asus\\anaconda3\\envs\\web-Pipeline\\lib\\site-packages\\datasets\\metric.py:494, in Metric.add_batch(self, predictions, references, **kwargs)\n",
    "    492 batch = {\"predictions\": predictions, \"references\": references, **kwargs}\n",
    "    493 batch = {intput_name: batch[intput_name] for intput_name in self.features}\n",
    "--> 494 batch = self.info.features.encode_batch(batch)\n",
    "    495 if self.writer is None:\n",
    "    496     self._init_writer()\n",
    "\n",
    "File c:\\Users\\asus\\anaconda3\\envs\\web-Pipeline\\lib\\site-packages\\datasets\\features\\features.py:1858, in Features.encode_batch(self, batch)\n",
    "   1856 for key, column in batch.items():\n",
    "   1857     column = cast_to_python_objects(column)\n",
    "-> 1858     encoded_batch[key] = [encode_nested_example(self[key], obj) for obj in column]\n",
    "...\n",
    "-> 1262     raise ValueError(f\"Got a string but expected a list instead: '{obj}'\")\n",
    "   1263 else:\n",
    "   1264     if len(obj) > 0:\n",
    "\n",
    "ValueError: Got a string but expected a list instead: 'hello world'\n",
    "Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...\n",
    "BLEU score: 0.22111541438690566\n",
    "Individual 1-gram: 0.370409\n",
    "Individual 2-gram: 0.329253\n",
    "Individual 3-gram: 0.185205\n",
    "Individual 4-gram: 0.105831\n",
    "Cumulative 2-gram: 0.349225\n",
    "Cumulative 3-gram: 0.285412\n",
    "Cumulative 4-gram: 0.221115\n",
    "BLEU score: 1.0270193092081295e-77\n",
    "Individual 1-gram: 1.000000\n",
    "Individual 2-gram: 1.000000\n",
    "Individual 3-gram: 0.500000\n",
    "Individual 4-gram: 0.000000\n",
    "BLEU score: 4.6144530849934175e-155\n",
    "Individual 1-gram: 0.367879\n",
    "Individual 2-gram: 0.183940\n",
    "Individual 3-gram: 0.000000\n",
    "Individual 4-gram: 0.000000\n",
    "Cumulative 2-gram: 0.260130\n",
    "Cumulative 3-gram: 0.000000\n",
    "Cumulative 4-gram: 0.000000\n",
    "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click here for more info. View Jupyter log for further details.\n",
    "---------------------------------------------------------------------------\n",
    "ModuleNotFoundError                       Traceback (most recent call last)\n",
    "Cell In[7], line 2\n",
    "      1 import nltk\n",
    "----> 2 from rouge import Rouge\n",
    "      4 gpt2_output = \"Seorang putri muda yang bernama Luna hidup dalam sebuah kerajaan besar yang dipimpin oleh raja dan ratu.Seorang putri muda yang bernama Luna hidup dalam sebuah kerajaan besar yang dipimpin oleh raja dan ratu. Luna adalah seorang gadis yang sangat menyukai petualangan. Suatu hari, dia mendengar kabar tentang sebuah hutan terlarang yang belum pernah dijelajahi oleh manusia sebelumnya. Dia merasa tertarik dan memutuskan untuk menjelajahi hutan itu sendirian. Namun, ketika dia tiba di sana, sebuah badai menghadang di depan matanya dan memaksanya untuk bertahan hidup dengan cara yang paling aman dan terbaik yang bisa dia lakukan.  Seorang wanita muda bernama Maya baru saja lulus kuliah dan memulai pekerjaan barunya sebagai editor buku di sebuah penerbit terkenal. Maya sedang mencari penerbit yang tepat untuk menerbitkan bukunya, tetapi dia tidak tahu harus mulai dari mana. Hal ini menyebabkan dia merasa kesulitan untuk memulai bisnis barunya dan pada akhirnya membuatnya merasa semakin tertekan dan semakin sulit untuk fokus pada pekerjaannya saat dia terus-menerus merasa gugup dan cemas setiap kali dia melangkah keluar dari gedung penerbitnya.  Maya segera menyadari bahwa ada sesuatu yang salah dengan dirinya dan dia harus berhati-hati dalam mengambil keputusan. Sesuatu yang tidak dia ketahui tentang dunia luar biasanya dan kehidupan orang-orang yang tinggal di dalamnya juga berada di luar jangkauannya. Selain itu, Maya juga mulai merasa takut dan terancam oleh makhluk-makhluk gaib yang berkeliaran di sekitar tempat dia tinggal dan berusaha untuk menguasai mereka. Untuk membantu mengatasi masalah-masalah yang dihadapi Maya, seorang penulisSeorang putri muda yang bernama Luna hidup dalam sebuah kerajaan besar yang dipimpin oleh raja dan ratu.\"\n",
    "      6 reference_sentences = \"Seorang putri muda yang bernama Luna hidup dalam sebuah kerajaan besar yang dipimpin oleh raja dan ratu.Seorang putri muda yang bernama Luna hidup dalam sebuah kerajaan besar yang dipimpin oleh raja dan ratu. Luna adalah seorang gadis yang sangat menyukai petualangan. Suatu hari, dia mendengar kabar tentang sebuah hutan terlarang yang belum pernah dijelajahi oleh manusia sebelumnya. Dia merasa tertarik dan memutuskan untuk menjelajahi hutan itu sendirian.  Namun, ketika dia tiba di sana, sebuah badai menghadang di depan matanya dan memaksanya untuk bertahan hidup dengan cara yang paling aman dan terbaik yang bisa dia lakukan.  Seorang wanita muda bernama Maya baru saja lulus kuliah dan memulai pekerjaan barunya sebagai editor buku di sebuah penerbit terkenal. Maya sedang mencari penerbit yang tepat untuk menerbitkan bukunya, tetapi dia tidak tahu harus mulai dari mana. Hal ini menyebabkan dia merasa kesulitan untuk memulai bisnis barunya dan pada akhirnya membuatnya merasa semakin tertekan dan semakin sulit untuk fokus pada pekerjaannya saat dia terus-menerus merasa gugup dan cemas setiap kali dia melangkah keluar dari gedung penerbitnya.  Maya segera menyadari bahwa ada sesuatu yang salah dengan dirinya dan dia harus berhati-hati dalam mengambil keputusan. Sesuatu yang tidak dia ketahui tentang dunia luar biasanya dan kehidupan orang-orang yang tinggal di dalamnya juga berada di luar jangkauannya. Selain itu, Maya juga mulai merasa takut dan terancam oleh makhluk-makhluk gaib yang berkeliaran di sekitar tempat dia tinggal dan berusaha untuk menguasai mereka. Untuk membantu mengatasi masalah-masalah yang dihadapi Maya, seorang penulisSeorang putri muda yang bernama Luna hidup dalam sebuah kerajaan besar yang dipimpin oleh raja dan ratu.\"\n",
    "\n",
    "ModuleNotFoundError: No module named 'rouge'\n",
    "---------------------------------------------------------------------------\n",
    "NameError                                 Traceback (most recent call last)\n",
    "Cell In[51], line 4\n",
    "      1 from rouge import FilesRouge\n",
    "      3 files_rouge = FilesRouge()\n",
    "----> 4 scores = files_rouge.get_scores(hyp_path, ref_path)\n",
    "      5 # or\n",
    "      6 scores = files_rouge.get_scores(hyp_path, ref_path, avg=True)\n",
    "\n",
    "NameError: name 'hyp_path' is not defined\n",
    "I loved reading the Hunger Games\n",
    "ROUGE Score: {'rouge-1': {'r': 1.0, 'p': 0.8571428571428571, 'f': 0.9230769181065088}, 'rouge-2': {'r': 0.8, 'p': 0.6666666666666666, 'f': 0.7272727223140496}, 'rouge-l': {'r': 1.0, 'p': 0.8571428571428571, 'f': 0.9230769181065088}}\n",
    "Requirement already satisfied: rouge-score in c:\\users\\asus\\anaconda3\\envs\\caret_testing\\lib\\site-packages (0.1.2)\n",
    "Requirement already satisfied: absl-py in c:\\users\\asus\\anaconda3\\envs\\caret_testing\\lib\\site-packages (from rouge-score) (2.0.0)\n",
    "Requirement already satisfied: nltk in c:\\users\\asus\\anaconda3\\envs\\caret_testing\\lib\\site-packages (from rouge-score) (3.8.1)\n",
    "Requirement already satisfied: numpy in c:\\users\\asus\\anaconda3\\envs\\caret_testing\\lib\\site-packages (from rouge-score) (1.23.5)\n",
    "Requirement already satisfied: six>=1.14.0 in c:\\users\\asus\\anaconda3\\envs\\caret_testing\\lib\\site-packages (from rouge-score) (1.16.0)\n",
    "Requirement already satisfied: click in c:\\users\\asus\\anaconda3\\envs\\caret_testing\\lib\\site-packages (from nltk->rouge-score) (8.1.7)\n",
    "Requirement already satisfied: joblib in c:\\users\\asus\\anaconda3\\envs\\caret_testing\\lib\\site-packages (from nltk->rouge-score) (1.3.2)\n",
    "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\asus\\anaconda3\\envs\\caret_testing\\lib\\site-packages (from nltk->rouge-score) (2023.10.3)\n",
    "Requirement already satisfied: tqdm in c:\\users\\asus\\anaconda3\\envs\\caret_testing\\lib\\site-packages (from nltk->rouge-score) (4.66.1)\n",
    "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\envs\\caret_testing\\lib\\site-packages (from click->nltk->rouge-score) (0.4.6)\n",
    "---------------------------------------------------------------------------\n",
    "ModuleNotFoundError                       Traceback (most recent call last)\n",
    "Cell In[78], line 1\n",
    "----> 1 from rouge_metric import PerlRouge\n",
    "      3 rouge = PerlRouge(rouge_n_max=3, rouge_l=True, rouge_w=True,\n",
    "      4     rouge_w_weight=1.2, rouge_s=True, rouge_su=True, skip_gap=4)\n",
    "      6 # Load summary results and evaluate\n",
    "\n",
    "ModuleNotFoundError: No module named 'rouge_metric'\n",
    "Cumulative 2-gram: 0.325669\n",
    "Cumulative 3-gram: 0.000000\n",
    "Cumulative 4-gram: 0.000000\n",
    "BLEU score: 8.512570894324454e-155\n",
    "\n",
    "ROUGE Scores:\n",
    "rouge1 Precision: 0.2\n",
    "rouge1 Recall: 0.2647058823529412\n",
    "rouge1 F1 Score: 0.2278481012658228\n",
    "rouge2 Precision: 0.022727272727272728\n",
    "rouge2 Recall: 0.030303030303030304\n",
    "rouge2 F1 Score: 0.025974025974025976\n",
    "rougeL Precision: 0.17777777777777778\n",
    "rougeL Recall: 0.23529411764705882\n",
    "rougeL F1 Score: 0.20253164556962028\n",
    "Best References:\n",
    "rouge1: <|startoftext|> Suatu hari Scott sedang berjalan pulang dari sekolah. <|sentence1to2|> Dia mendengar suara yang datang dari semak -semak. <|sentence2to3|> Dia pergi lebih dekat untuk memeriksanya. <|sentence3to4|> Itu adalah anak anjing kecil tanpa kerah. <|sentence4to5|> Scott memutuskan untuk membawa pulang anak anjing itu dan membuatnya sendiri. <|endoftext|>\n",
    "rouge1 Precision: 0.2962962962962963\n",
    "rouge1 Recall: 0.17777777777777778\n",
    "rouge1 F1 Score: 0.2222222222222222\n",
    "\n",
    "rouge2: <|startoftext|> Suatu hari Scott sedang berjalan pulang dari sekolah. <|sentence1to2|> Dia mendengar suara yang datang dari semak -semak. <|sentence2to3|> Dia pergi lebih dekat untuk memeriksanya. <|sentence3to4|> Itu adalah anak anjing kecil tanpa kerah. <|sentence4to5|> Scott memutuskan untuk membawa pulang anak anjing itu dan membuatnya sendiri. <|endoftext|>\n",
    "rouge2 Precision: 0.038461538461538464\n",
    "rouge2 Recall: 0.022727272727272728\n",
    "rouge2 F1 Score: 0.028571428571428574\n",
    "\n",
    "rougeL: <|startoftext|> Laura sedang berjalan pulang dari sekolah. <|sentence1to2|> Dia mendengar suara di belakangnya. <|sentence2to3|> Dia melihat ke belakang. <|sentence3to4|> Dia melihat bukan apa -apa. <|sentence4to5|> Laura diam -diam diikuti oleh seorang pria misterius. <|endoftext|>\n",
    "rougeL Precision: 0.25925925925925924\n",
    "rougeL Recall: 0.15555555555555556\n",
    "rougeL F1 Score: 0.19444444444444445\n",
    "\n",
    "Cumulative 2-gram: 0.574684\n",
    "Cumulative 3-gram: 0.509943\n",
    "Cumulative 4-gram: 0.444460\n",
    "BLEU score: 0.444459729828027\n",
    "BLEU score: 8.90735602648238e-232\n",
    "\n",
    "Best-matching reference:\n",
    "<\n",
    "c:\\Users\\asus\\anaconda3\\envs\\caret_testing\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
    "The hypothesis contains 0 counts of 2-gram overlaps.\n",
    "Therefore the BLEU score evaluates to 0, independently of\n",
    "how many N-gram overlaps of lower order it contains.\n",
    "Consider using lower n-gram order or use SmoothingFunction()\n",
    "  warnings.warn(_msg)\n",
    "c:\\Users\\asus\\anaconda3\\envs\\caret_testing\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
    "The hypothesis contains 0 counts of 3-gram overlaps.\n",
    "Therefore the BLEU score evaluates to 0, independently of\n",
    "how many N-gram overlaps of lower order it contains.\n",
    "Consider using lower n-gram order or use SmoothingFunction()\n",
    "  warnings.warn(_msg)\n",
    "rouge1 Precision: 0.5454545454545454\n",
    "rouge1 Recall: 0.5454545454545454\n",
    "rouge1 F1 Score: 0.5454545454545454\n",
    "\n",
    "rouge2 Precision: 0.3\n",
    "rouge2 Recall: 0.3\n",
    "rouge2 F1 Score: 0.3\n",
    "\n",
    "rougeL Precision: 0.5454545454545454\n",
    "rougeL Recall: 0.5454545454545454\n",
    "rougeL F1 Score: 0.5454545454545454\n",
    "\n",
    "Cumulative 2-gram: 0.866025\n",
    "Cumulative 3-gram: 0.768352\n",
    "Cumulative 4-gram: 0.668740\n",
    "BLEU score: 0.668740304976422\n",
    "Cumulative 2-gram: 0.715566\n",
    "Cumulative 3-gram: 0.661127\n",
    "Cumulative 4-gram: 0.598801\n",
    "BLEU score: 0.5988005945384479\n",
    "rouge1 Precision: 0.33962264150943394\n",
    "rouge1 Recall: 0.4\n",
    "rouge1 F1 Score: 0.36734693877551017\n",
    "\n",
    "rouge2 Precision: 0.17307692307692307\n",
    "rouge2 Recall: 0.20454545454545456\n",
    "rouge2 F1 Score: 0.18750000000000003\n",
    "\n",
    "rougeL Precision: 0.24528301886792453\n",
    "rougeL Recall: 0.28888888888888886\n",
    "rougeL F1 Score: 0.26530612244897955\n",
    "\n",
    "Best Reference:\n",
    "<|startoftext|> Laura sedang berjalan pulang dari sekolah. <|sentence1to2|> Dia mendengar suara di belakangnya. <|sentence2to3|> Dia melihat ke belakang. <|sentence3to4|> Dia melihat bukan apa -apa. <|sentence4to5|> Laura diam -diam diikuti oleh seorang pria misterius. <|endoftext|>\n",
    "\n",
    "ROUGE Scores:\n",
    "ROUGE-1 Precision: 0.2962962962962963\n",
    "ROUGE-1 Recall: 0.17777777777777778\n",
    "ROUGE-1 F1 Score: 0.2222222222222222\n",
    "ROUGE-2 Precision: 0.038461538461538464\n",
    "ROUGE-2 Recall: 0.022727272727272728\n",
    "ROUGE-2 F1 Score: 0.028571428571428574\n",
    "ROUGE-L Precision: 0.25925925925925924\n",
    "ROUGE-L Recall: 0.15555555555555556\n",
    "ROUGE-L F1 Score: 0.19444444444444445\n",
    "c:\\Users\\asus\\anaconda3\\envs\\Web-Transformers-Nvidia\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
    "The hypothesis contains 0 counts of 4-gram overlaps.\n",
    "Therefore the BLEU score evaluates to 0, independently of\n",
    "how many N-gram overlaps of lower order it contains.\n",
    "Consider using lower n-gram order or use SmoothingFunction()\n",
    "  warnings.warn(_msg)\n",
    "Best BLEU Reference:\n",
    "<|startoftext|> Suatu hari Scott sedang berjalan pulang dari sekolah. <|sentence1to2|> Dia mendengar suara yang datang dari semak -semak. <|sentence2to3|> Dia pergi lebih dekat untuk memeriksanya. <|sentence3to4|> Itu adalah anak anjing kecil tanpa kerah. <|sentence4to5|> Scott memutuskan untuk membawa pulang anak anjing itu dan membuatnya sendiri. <|endoftext|>\n",
    "\n",
    "BLEU Score: 0.42277523300671854\n",
    "Best References:\n",
    "rouge1: ['<|startoftext|> Suatu hari Scott sedang berjalan pulang dari sekolah. <|sentence1to2|> Dia mendengar suara yang datang dari semak -semak. <|sentence2to3|> Dia pergi lebih dekat untuk memeriksanya. <|sentence3to4|> Itu adalah anak anjing kecil tanpa kerah. <|sentence4to5|> Scott memutuskan untuk membawa pulang anak anjing itu dan membuatnya sendiri. <|endoftext|>',\n",
    "          '<|startoftext|> Laura sedang berjalan pulang dari sekolah. <|sentence1to2|> Dia mendengar suara di belakangnya. <|sentence2to3|> Dia melihat ke belakang. <|sentence3to4|> Dia melihat bukan apa -apa. <|sentence4to5|> Laura diam -diam diikuti oleh seorang pria misterius. <|endoftext|>', \n",
    "         '<|startoftext|> Shelly membiarkan anjingnya keluar di halaman belakang untuk bermain. <|sentence1to2|> Beberapa saat kemudian dia melihat seekor groundhog mengintip dari bawah gudang. <|sentence2to3|> Dia memanggil anjing -anjing itu kembali ke dalam sebelum mereka melihat makhluk itu. <|sentence3to4|> Begitu masuk, anjing melihat groundhog dan mulai menggonggong. <|sentence4to5|> Groundhog kecil berlari di bawah pagar ke halaman tetangga. <|endoftext|>', \n",
    "         '<|startoftext|> Bill sedang berjalan pulang dari kantor. <|sentence1to2|> Dan seorang pria tunawisma memintanya uang. <|sentence2to3|> Bill memberi pria itu beberapa dolar. <|sentence3to4|> Tapi dia tidak nyaman dengan seberapa dekat pria itu datang kepadanya. <|sentence4to5|> Bill mulai merasa sakit setelah dia tiba di rumah. <|endoftext|>']\n",
    "rouge1 Precision: 0.07407407407407407\n",
    "rouge1 Recall: 0.05128205128205128\n",
    "rouge1 F1 Score: 0.060606060606060615\n",
    "\n",
    "rouge2: ['<|startoftext|> Laura sedang berjalan pulang dari sekolah. <|sentence1to2|> Dia mendengar suara di belakangnya. <|sentence2to3|> Dia melihat ke belakang. <|sentence3to4|> Dia melihat bukan apa -apa. <|sentence4to5|> Laura diam -diam diikuti oleh seorang pria misterius. <|endoftext|>', \n",
    "         '<|startoftext|> Aaron takut ular. <|sentence1to2|> Ketika dia pulang dari sekolah, dia melihat seekor ular di halaman. <|sentence2to3|> Dia mulai gemetar dan menangis. <|sentence3to4|> Ayahnya pulang dan menyingkirkan ular itu. <|sentence4to5|> Butuh beberapa saat bagi Aaron untuk tenang. <|endoftext|>', \n",
    "         '<|startoftext|> Tina sedang berjalan di dekat rel kereta api. <|sentence1to2|> Dia melihat seorang pria tunawisma goyah di trek. <|sentence2to3|> Dia jelas mabuk, kereta api yang datang ke kejauhan. <|sentence3to4|> Dia berlari ke arahnya, berteriak dan melambaikan tangannya. <|sentence4to5|> Dengan bersendawa, dia tersandung dari trek dan pingsan di parit. <|endoftext|>',\n",
    "           '<|startoftext|> Suatu hari Scott sedang berjalan pulang dari sekolah. <|sentence1to2|> Dia mendengar suara yang datang dari semak -semak. <|sentence2to3|> Dia pergi lebih dekat untuk memeriksanya. <|sentence3to4|> Itu adalah anak anjing kecil tanpa kerah. <|sentence4to5|> Scott memutuskan untuk membawa pulang anak anjing itu dan membuatnya sendiri. <|endoftext|>']\n",
    "rouge2 Precision: 0.0\n",
    "rouge2 Recall: 0.0\n",
    "rouge2 F1 Score: 0.0\n",
    "\n",
    "rougeL: ['<|startoftext|> Suatu hari Scott sedang berjalan pulang dari sekolah. <|sentence1to2|> Dia mendengar suara yang datang dari semak -semak. <|sentence2to3|> Dia pergi lebih dekat untuk memeriksanya. <|sentence3to4|> Itu adalah anak anjing kecil tanpa kerah. <|sentence4to5|> Scott memutuskan untuk membawa pulang anak anjing itu dan membuatnya sendiri. <|endoftext|>', '<|startoftext|> Tim membeli anjing baru. <|sentence1to2|> Suatu hari dia berjalan anjing di hutan besar. <|sentence2to3|> Anjing itu berlari ke semak -semak. <|sentence3to4|> Tim mencari satu jam mencoba menemukan anjingnya. <|sentence4to5|> Anjing itu akhirnya muncul membawa tongkat terbesar yang pernah ada. <|endoftext|>', '<|startoftext|> Billy berjalan di jalan suatu hari. <|sentence1to2|> Dia melihat seekor kucing terjebak di pohon. <|sentence2to3|> Dia mencoba menyebutnya. <|sentence3to4|> Kucing itu takut dan tidak datang kepadanya. <|sentence4to5|> Dia memanjat pohon itu dan menyelamatkan kucing itu sendiri. <|endoftext|>', '<|startoftext|> Laura sedang berjalan pulang dari sekolah. <|sentence1to2|> Dia mendengar suara di belakangnya. <|sentence2to3|> Dia melihat ke belakang. <|sentence3to4|> Dia melihat bukan apa -apa. <|sentence4to5|> Laura diam -diam diikuti oleh seorang pria misterius. <|endoftext|>']\n",
    "rougeL Precision: 0.07407407407407407\n",
    "rougeL Recall: 0.05128205128205128\n",
    "rougeL F1 Score: 0.060606060606060615"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformers-pandas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
