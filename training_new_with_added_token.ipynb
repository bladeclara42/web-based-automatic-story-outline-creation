{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import TextDataset,LineByLineTextDataset,DataCollatorForLanguageModeling\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>storyid</th>\n",
       "      <th>storytitle</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>sentence3</th>\n",
       "      <th>sentence4</th>\n",
       "      <th>sentence5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f3643541-c339-4377-851d-144ea400d551</td>\n",
       "      <td>Ibu tiri</td>\n",
       "      <td>Eric dan istrinya memiliki seorang putri berna...</td>\n",
       "      <td>Istri Eric meninggal.</td>\n",
       "      <td>Eric dan Meg sangat sedih.</td>\n",
       "      <td>Eric bertemu dengan seorang wanita dan menikah...</td>\n",
       "      <td>Meg sekarang senang dengan ibu tirinya yang baru.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d61b69c7-7eca-4099-ade2-1a7e5a014a13</td>\n",
       "      <td>Riasan yang buruk</td>\n",
       "      <td>Marnie akan menikah.</td>\n",
       "      <td>Untuk pernikahannya, dia perlu makeup selesai.</td>\n",
       "      <td>Akibatnya dia menyewa penata rias untuk memban...</td>\n",
       "      <td>Ketika penata rias selesai, Marinir melihatnya.</td>\n",
       "      <td>Dia ngeri dan akhirnya memecatnya,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3ea38bec-1101-4a64-86fa-948904d33a2b</td>\n",
       "      <td>Bersatu kembali di reuni</td>\n",
       "      <td>Tom selalu bertanya -tanya apakah dia pernah m...</td>\n",
       "      <td>Dia telah memutuskan untuk menikahi orang lain...</td>\n",
       "      <td>Sekarang, dia bercerai dan akan datang ke reun...</td>\n",
       "      <td>Tom dan Sue menghabiskan sebagian besar reuni ...</td>\n",
       "      <td>Kedua kekasih yang bersatu kembali telah bersa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f085aab6-c13a-46d6-aca0-cf942b50577a</td>\n",
       "      <td>Keluarga baru</td>\n",
       "      <td>Jenn dan Bill jatuh cinta dan menikah.</td>\n",
       "      <td>Beberapa bulan kemudian, Jenn mengetahui bahwa...</td>\n",
       "      <td>Bill sangat bersemangat dan tidak sabar untuk ...</td>\n",
       "      <td>Mereka mengetahui bahwa mereka memiliki bayi l...</td>\n",
       "      <td>Begitu bayi itu lahir, mereka menamainya Theod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e19867fe-9b77-4a56-934a-9fd00d4c802e</td>\n",
       "      <td>Lone Girl</td>\n",
       "      <td>Dessie ingin bertemu pasangan baru.</td>\n",
       "      <td>Dia pergi melalui banyak situs kencan tidak be...</td>\n",
       "      <td>Akhirnya, dia menyerah dan pergi ke perpustakaan.</td>\n",
       "      <td>Anehnya, dia bertemu dengan seorang pria tampa...</td>\n",
       "      <td>Setelah beberapa minggu, mereka bersama dalam ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>6cb171ed-99f3-46f1-8d98-5dad776d9555</td>\n",
       "      <td>Wahyu Mike</td>\n",
       "      <td>Mike menunda meminta pacarnya untuk menikah de...</td>\n",
       "      <td>Dia telah mengatakan kepadanya bahwa dia ingin...</td>\n",
       "      <td>Sekarang kuliah itu selesai, dia merasa tertek...</td>\n",
       "      <td>Sayangnya untuk Mike, keluar bahwa dia tidak i...</td>\n",
       "      <td>Sekarang Mike menyadari bahwa dia seharusnya j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>4de38ecd-39a3-4c7c-988e-93e6f8ec7705</td>\n",
       "      <td>Ultimatum Karen</td>\n",
       "      <td>Karen telah bersama pacarnya selama bertahun -...</td>\n",
       "      <td>Sekarang adik laki -lakinya bertunangan dan me...</td>\n",
       "      <td>Karen merasa sudah waktunya pacarnya untuk ber...</td>\n",
       "      <td>Dia memberinya ultimatum untuk menikah atau pe...</td>\n",
       "      <td>Pacarnya memilih untuk pergi.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>7ce8f342-d590-40de-a312-aec4a638dcc9</td>\n",
       "      <td>Pernikahan atau kami putus</td>\n",
       "      <td>Janet dan John telah berkencan selama 10 tahun.</td>\n",
       "      <td>Janet selalu ingin menikah dengan John.</td>\n",
       "      <td>Tapi John tidak pernah ingin menikah.</td>\n",
       "      <td>Janet memberi tahu John bahwa mereka akan meni...</td>\n",
       "      <td>Sayangnya untuk Janet, John memilih bahwa mere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>31be3d5e-1c96-4c46-92e1-70bb1a888ee6</td>\n",
       "      <td>Bayi YouTube</td>\n",
       "      <td>Latoya dan Adam bertemu di acara YouTube.</td>\n",
       "      <td>Mereka berdua memiliki saluran YouTube sendiri.</td>\n",
       "      <td>Keduanya memiliki chemistry yang hebat dan mul...</td>\n",
       "      <td>Setahun kemudian mereka pindah bersama dan men...</td>\n",
       "      <td>Mereka telah bersama selama 5 tahun sekarang d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>2c14252b-4080-4fca-8765-537772018508</td>\n",
       "      <td>Jamie menikahi cinta</td>\n",
       "      <td>Jamie adalah seorang gadis Amerika.</td>\n",
       "      <td>Jamie ingin menikah dengan pria Meksiko.</td>\n",
       "      <td>Keluarganya menganggap itu karena pria itu men...</td>\n",
       "      <td>Jamie bersikeras bahwa dia menikahinya karena ...</td>\n",
       "      <td>Jamie menikah dan mereka menghabiskan sisa hid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>938 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  storyid                  storytitle  \\\n",
       "0    f3643541-c339-4377-851d-144ea400d551                    Ibu tiri   \n",
       "1    d61b69c7-7eca-4099-ade2-1a7e5a014a13           Riasan yang buruk   \n",
       "2    3ea38bec-1101-4a64-86fa-948904d33a2b    Bersatu kembali di reuni   \n",
       "3    f085aab6-c13a-46d6-aca0-cf942b50577a               Keluarga baru   \n",
       "4    e19867fe-9b77-4a56-934a-9fd00d4c802e                   Lone Girl   \n",
       "..                                    ...                         ...   \n",
       "933  6cb171ed-99f3-46f1-8d98-5dad776d9555                  Wahyu Mike   \n",
       "934  4de38ecd-39a3-4c7c-988e-93e6f8ec7705             Ultimatum Karen   \n",
       "935  7ce8f342-d590-40de-a312-aec4a638dcc9  Pernikahan atau kami putus   \n",
       "936  31be3d5e-1c96-4c46-92e1-70bb1a888ee6                Bayi YouTube   \n",
       "937  2c14252b-4080-4fca-8765-537772018508        Jamie menikahi cinta   \n",
       "\n",
       "                                             sentence1  \\\n",
       "0    Eric dan istrinya memiliki seorang putri berna...   \n",
       "1                                 Marnie akan menikah.   \n",
       "2    Tom selalu bertanya -tanya apakah dia pernah m...   \n",
       "3               Jenn dan Bill jatuh cinta dan menikah.   \n",
       "4                  Dessie ingin bertemu pasangan baru.   \n",
       "..                                                 ...   \n",
       "933  Mike menunda meminta pacarnya untuk menikah de...   \n",
       "934  Karen telah bersama pacarnya selama bertahun -...   \n",
       "935    Janet dan John telah berkencan selama 10 tahun.   \n",
       "936          Latoya dan Adam bertemu di acara YouTube.   \n",
       "937                Jamie adalah seorang gadis Amerika.   \n",
       "\n",
       "                                             sentence2  \\\n",
       "0                                Istri Eric meninggal.   \n",
       "1       Untuk pernikahannya, dia perlu makeup selesai.   \n",
       "2    Dia telah memutuskan untuk menikahi orang lain...   \n",
       "3    Beberapa bulan kemudian, Jenn mengetahui bahwa...   \n",
       "4    Dia pergi melalui banyak situs kencan tidak be...   \n",
       "..                                                 ...   \n",
       "933  Dia telah mengatakan kepadanya bahwa dia ingin...   \n",
       "934  Sekarang adik laki -lakinya bertunangan dan me...   \n",
       "935            Janet selalu ingin menikah dengan John.   \n",
       "936    Mereka berdua memiliki saluran YouTube sendiri.   \n",
       "937           Jamie ingin menikah dengan pria Meksiko.   \n",
       "\n",
       "                                             sentence3  \\\n",
       "0                           Eric dan Meg sangat sedih.   \n",
       "1    Akibatnya dia menyewa penata rias untuk memban...   \n",
       "2    Sekarang, dia bercerai dan akan datang ke reun...   \n",
       "3    Bill sangat bersemangat dan tidak sabar untuk ...   \n",
       "4    Akhirnya, dia menyerah dan pergi ke perpustakaan.   \n",
       "..                                                 ...   \n",
       "933  Sekarang kuliah itu selesai, dia merasa tertek...   \n",
       "934  Karen merasa sudah waktunya pacarnya untuk ber...   \n",
       "935              Tapi John tidak pernah ingin menikah.   \n",
       "936  Keduanya memiliki chemistry yang hebat dan mul...   \n",
       "937  Keluarganya menganggap itu karena pria itu men...   \n",
       "\n",
       "                                             sentence4  \\\n",
       "0    Eric bertemu dengan seorang wanita dan menikah...   \n",
       "1      Ketika penata rias selesai, Marinir melihatnya.   \n",
       "2    Tom dan Sue menghabiskan sebagian besar reuni ...   \n",
       "3    Mereka mengetahui bahwa mereka memiliki bayi l...   \n",
       "4    Anehnya, dia bertemu dengan seorang pria tampa...   \n",
       "..                                                 ...   \n",
       "933  Sayangnya untuk Mike, keluar bahwa dia tidak i...   \n",
       "934  Dia memberinya ultimatum untuk menikah atau pe...   \n",
       "935  Janet memberi tahu John bahwa mereka akan meni...   \n",
       "936  Setahun kemudian mereka pindah bersama dan men...   \n",
       "937  Jamie bersikeras bahwa dia menikahinya karena ...   \n",
       "\n",
       "                                             sentence5  \n",
       "0    Meg sekarang senang dengan ibu tirinya yang baru.  \n",
       "1                   Dia ngeri dan akhirnya memecatnya,  \n",
       "2    Kedua kekasih yang bersatu kembali telah bersa...  \n",
       "3    Begitu bayi itu lahir, mereka menamainya Theod...  \n",
       "4    Setelah beberapa minggu, mereka bersama dalam ...  \n",
       "..                                                 ...  \n",
       "933  Sekarang Mike menyadari bahwa dia seharusnya j...  \n",
       "934                      Pacarnya memilih untuk pergi.  \n",
       "935  Sayangnya untuk Janet, John memilih bahwa mere...  \n",
       "936  Mereka telah bersama selama 5 tahun sekarang d...  \n",
       "937  Jamie menikah dan mereka menghabiskan sisa hid...  \n",
       "\n",
       "[938 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_dataset = pd.read_csv('translated_romance_2.csv')\n",
    "translated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebelum\n",
      "['<|endoftext|>']\n",
      "[50256]\n",
      "Vocabulary size: 50257\n",
      "Pad token ID: None\n",
      "Setelah\n",
      "['<|endoftext|>', '<|firstoftext|>', '<|lastoftext|>', '<|sentence1to2|>', '<|sentence2to3|>', '<|sentence3to4|>', '<|sentence4to5|>']\n",
      "[50256, 50257, 50258, 50259, 50260, 50261, 50262]\n",
      "Vocabulary size: 50257\n",
      "Pad token ID: None\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"indonesian-nlp/gpt2\")\n",
    "print(\"Sebelum\")\n",
    "print(tokenizer.all_special_tokens)\n",
    "print(tokenizer.all_special_ids)\n",
    "print(\"Vocabulary size:\", tokenizer.vocab_size)\n",
    "print(\"Pad token ID:\", tokenizer.pad_token_id)\n",
    "\n",
    "special_tokens_dict = {'additional_special_tokens': ['<|firstoftext|>','<|lastoftext|>', '<|sentence1to2|>', '<|sentence2to3|>', '<|sentence3to4|>', '<|sentence4to5|>']}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "print(\"Setelah\")\n",
    "print(tokenizer.all_special_tokens)\n",
    "print(tokenizer.all_special_ids)  \n",
    "print(\"Vocabulary size:\", tokenizer.vocab_size)\n",
    "print(\"Pad token ID:\", tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Text generation using GPT-2 (new dataset)\\training_new_with_added_token.ipynb Cell 4\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Text%20generation%20using%20GPT-2%20%28new%20dataset%29/training_new_with_added_token.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m sequences \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mThis is a sequence.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mAnother sequence.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mYet another one.\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Text%20generation%20using%20GPT-2%20%28new%20dataset%29/training_new_with_added_token.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Tokenize the sequences\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Text%20generation%20using%20GPT-2%20%28new%20dataset%29/training_new_with_added_token.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m tokenized_sequences \u001b[39m=\u001b[39m tokenizer(sequences, padding\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Text%20generation%20using%20GPT-2%20%28new%20dataset%29/training_new_with_added_token.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Print the padded sequences\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Text%20generation%20using%20GPT-2%20%28new%20dataset%29/training_new_with_added_token.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(tokenized_sequences)\n",
      "File \u001b[1;32mc:\\Users\\asus\\anaconda3\\envs\\Transformers-pandas\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2602\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2600\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2601\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2602\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_one(text\u001b[39m=\u001b[39mtext, text_pair\u001b[39m=\u001b[39mtext_pair, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mall_kwargs)\n\u001b[0;32m   2603\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2604\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32mc:\\Users\\asus\\anaconda3\\envs\\Transformers-pandas\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2688\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2683\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2684\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch length of `text`: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text)\u001b[39m}\u001b[39;00m\u001b[39m does not match batch length of `text_pair`:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2685\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text_pair)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2686\u001b[0m         )\n\u001b[0;32m   2687\u001b[0m     batch_text_or_text_pairs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(text, text_pair)) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m text\n\u001b[1;32m-> 2688\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m   2689\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2690\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2691\u001b[0m         padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   2692\u001b[0m         truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   2693\u001b[0m         max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   2694\u001b[0m         stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m   2695\u001b[0m         is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2696\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2697\u001b[0m         return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2698\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2699\u001b[0m         return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2700\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2701\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2702\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2703\u001b[0m         return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m   2704\u001b[0m         verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   2705\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2706\u001b[0m     )\n\u001b[0;32m   2707\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2708\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[0;32m   2709\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[0;32m   2710\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2726\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2727\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\asus\\anaconda3\\envs\\Transformers-pandas\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2870\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2853\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2854\u001b[0m \u001b[39mTokenize and prepare for the model a list of sequences or a list of pairs of sequences.\u001b[39;00m\n\u001b[0;32m   2855\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2866\u001b[0m \u001b[39m        details in `encode_plus`).\u001b[39;00m\n\u001b[0;32m   2867\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2869\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m-> 2870\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   2871\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   2872\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   2873\u001b[0m     max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   2874\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2875\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   2876\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2877\u001b[0m )\n\u001b[0;32m   2879\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_encode_plus(\n\u001b[0;32m   2880\u001b[0m     batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2881\u001b[0m     add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2896\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2897\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\asus\\anaconda3\\envs\\Transformers-pandas\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2507\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._get_padding_truncation_strategies\u001b[1;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2505\u001b[0m \u001b[39m# Test if we have a padding token\u001b[39;00m\n\u001b[0;32m   2506\u001b[0m \u001b[39mif\u001b[39;00m padding_strategy \u001b[39m!=\u001b[39m PaddingStrategy\u001b[39m.\u001b[39mDO_NOT_PAD \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad_token \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad_token_id \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[1;32m-> 2507\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2508\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsking to pad but the tokenizer does not have a padding token. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2509\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2510\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor add a new pad token via `tokenizer.add_special_tokens(\u001b[39m\u001b[39m{\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpad_token\u001b[39m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m[PAD]\u001b[39m\u001b[39m'\u001b[39m\u001b[39m})`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2511\u001b[0m     )\n\u001b[0;32m   2513\u001b[0m \u001b[39m# Check that we will truncate to a multiple of pad_to_multiple_of if both are provided\u001b[39;00m\n\u001b[0;32m   2514\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   2515\u001b[0m     truncation_strategy \u001b[39m!=\u001b[39m TruncationStrategy\u001b[39m.\u001b[39mDO_NOT_TRUNCATE\n\u001b[0;32m   2516\u001b[0m     \u001b[39mand\u001b[39;00m padding_strategy \u001b[39m!=\u001b[39m PaddingStrategy\u001b[39m.\u001b[39mDO_NOT_PAD\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2519\u001b[0m     \u001b[39mand\u001b[39;00m (max_length \u001b[39m%\u001b[39m pad_to_multiple_of \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[0;32m   2520\u001b[0m ):\n",
      "\u001b[1;31mValueError\u001b[0m: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"indonesian-nlp/gpt2\")\n",
    "\n",
    "# Your input sequences\n",
    "sequences = [\"This is a sequence.\", \"Another sequence.\", \"Yet another one.\"]\n",
    "\n",
    "# Tokenize the sequences\n",
    "tokenized_sequences = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Print the padded sequences\n",
    "print(tokenized_sequences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformers-pandas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
